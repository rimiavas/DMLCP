{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I06LmUlDIrKz",
    "outputId": "557bdeef-ae08-4cee-92ce-9afc46bbb34a"
   },
   "source": [
    "# Language Models 1\n",
    "\n",
    "## 1. Inference\n",
    "\n",
    "See [here](https://huggingface.co/tasks/text-generation) and [here](https://huggingface.co/docs/transformers/generation_strategies).\n",
    "\n",
    "## Workflow\n",
    "\n",
    "#### Drive\n",
    "\n",
    "If you need to load/save to your drive:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "import os\n",
    "os.chdir('drive/My Drive/IS53055B-DMLCP/DMLCP/python') # to change to another directory\n",
    "```\n",
    "\n",
    "#### Huggingface login\n",
    "\n",
    "For some models and datasets, and if you want to push your model to HF (same as GitHub, but for models) you need to be logged into your HF account.\n",
    "\n",
    "For that, you need to create an account [here](https://huggingface.co/) and then to ['/settings/tokens'](https://huggingface.co/settings/tokens) to create an access token.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from huggingface_hub import notebook_login\n",
    "if not (Path.home()/'.huggingface'/'token').exists():\n",
    "    notebook_login()\n",
    "```\n",
    "\n",
    "#### Install\n",
    "\n",
    "1. On Colab, you need to install `transformers`:\n",
    "\n",
    "```python\n",
    "!pip install -Uq transformers\n",
    "```\n",
    "\n",
    "2. Locally, I recommend creating a new environment when working with Huggingface, simply because it'll be faster and because the preferred library behind HF is PyTorch, which can conflict with TensorFlow... I detailed the steps [in the PyTorch part of `setup.md`](https://github.com/jchwenger/DMLCP/blob/main/setup.md#pytorch--huggingfacegradio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URjvsuUyIthY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "# See: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html#creating-models\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap # The textwrap module automatically formats text for you\n",
    "\n",
    "tw = textwrap.TextWrapper(   # many more options, see them with textwrap.TextWrapper?\n",
    "    width=79,                # the formatted width we want\n",
    "    replace_whitespace=False # this will keep whitespace & line breaks in the original text\n",
    ")\n",
    "\n",
    "def wrap_print(s):\n",
    "    \"\"\"Format text into Textwrapped lines and print it\"\"\"\n",
    "    print(\"\\n\".join(tw.wrap(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9Tf3fn2Z_DV"
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    'text-generation', # the specific task, which is also the tag on huggingface\n",
    "    model='gpt2',      # so many more models here: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
    "    device=device           # the default is just cpu, see here: https://huggingface.co/docs/transformers/pipeline_tutorial#device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.from_pretrained.example) for an example using `GenerationConfig` and [here](https://github.com/huggingface/transformers/issues/19853#issuecomment-1290759818) for the `pad_token_id` fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sueT3i6MfM8d"
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(\"gpt2\")\n",
    "generation_config.pad_token_id = generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVIs-VhOQN6O"
   },
   "source": [
    "The Huggingface is transitioning towards the use of generation config files (good for automation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fZaJdUnPg7E"
   },
   "outputs": [],
   "source": [
    "generation_config.max_length = 25\n",
    "generation_config.do_sample = True\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.temperature = .9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW2UOpO2QAFD"
   },
   "source": [
    "### Quick vocab note:\n",
    "\n",
    "`bos`: beginning of sentence  \n",
    "`eos`: end of sentence  \n",
    "`pad`: padding\n",
    "\n",
    "These are special tokens that have been inserted into the text at training time.\n",
    "\n",
    "For instance, in our case the 'beginning' of the text is 'endoftext', as during training the texts are fed to the network one after the other, with this special token between them:\n",
    "```python\n",
    "print(generator.tokenizer.bos_token) # '<|endoftext|>'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4NioPo5fSVu",
    "outputId": "84d70a38-4eee-4031-bac1-14a85467d6c7"
   },
   "outputs": [],
   "source": [
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61kgReONYBT5",
    "outputId": "cd2bae9a-2feb-44d7-c977-e371fea14c38"
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "generator(\n",
    "    \"Once upon a time,\",\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McOEE98ITaoz"
   },
   "source": [
    "Parallel generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40-Zwu0ETWnW",
    "outputId": "34296884-51a8-46ff-baba-9ac0e1a8ff97"
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "generator(\n",
    "    [\"Once upon a time,\"] * 2,\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3s8ntFfeB30"
   },
   "source": [
    "---\n",
    "\n",
    "## Deeper\n",
    "\n",
    "What does the pipeline do under the hood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EzS8SV5ccuu"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    pad_token_id=tokenizer.eos_token_id # add the EOS token as PAD token to avoid warnings\n",
    ").to(device) # to GPU/MPS/CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6_9iKN7Wqps"
   },
   "source": [
    "### Note\n",
    "\n",
    "Huggingface automates everything, so instead of `GPT2LMHeadModel` and `GPT2Tokenizer` you can use `AutoModelForCausalLM`, `AutoTokenizer`.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "```\n",
    "\n",
    "The automation of the right choice of model architecture by the Huggingface library has become so popular that these autoclasses are used almost everywhere now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXJjd_kEREkS"
   },
   "source": [
    "### The tokenizer\n",
    "\n",
    "See [the Preprocess](https://huggingface.co/docs/transformers/preprocessing) tutorial on Huggingface for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddqPQPE_SEOA",
    "outputId": "d0414e3d-8713-4948-b39a-2a5a6ba48419"
   },
   "outputs": [],
   "source": [
    "toks = tokenizer.encode(\"Oh sweet midnight\")\n",
    "print(toks)\n",
    "print(tokenizer.decode(toks))\n",
    "print()\n",
    "\n",
    "toks = tokenizer([\"Oh sweet midnight\", \"harbinger of doom\"])\n",
    "print(toks)\n",
    "print(tokenizer.batch_decode(toks['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gju_L3gKVN4j",
    "outputId": "3914d075-337b-4850-a223-6fb6ac0cf7ad"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('Once upon a time', return_tensors='pt') # pytorch tensors\n",
    "print(input_ids)\n",
    "\n",
    "batched_input_ids = torch.tile(input_ids, (4,1)).to(device) # just copying the tensor 4 times\n",
    "print(batched_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVKQUtN9eEpW"
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('Once upon a time', return_tensors='pt') # pytorch tensors\n",
    "\n",
    "batched_input_ids = torch.tile(input_ids, (4,1)).to(device) # copy and place on GPU/MPS/CPU\n",
    "\n",
    "# same logic as before\n",
    "generation_config = GenerationConfig.from_pretrained(\"gpt2\")\n",
    "generation_config.pad_token_id = generation_config.eos_token_id\n",
    "generation_config.max_length = 25\n",
    "generation_config.do_sample = True\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.temperature = .9\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "output = model.generate(\n",
    "    batched_input_ids, # try input_ids as well for a single strand\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibq2IrdhhSAH",
    "outputId": "4f4d119b-a32b-440a-d3ec-144a83d48e1f"
   },
   "outputs": [],
   "source": [
    "texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "for t in texts:\n",
    "    wrap_print(t)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U18eRLBbXAot"
   },
   "source": [
    "---\n",
    "\n",
    "# Experiments\n",
    "\n",
    "1. Test everything! Make sure you understand and develop an intuition of:\n",
    " - the various parameters: `temperature`, `top_k`, `top_p`;\n",
    " - the `tokenizer` object to convert text into tokens and back;\n",
    " - how to handle the whole pipeline;\n",
    "   Also, you can search for different [models](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)! (Some of them may exceed your GPU capacity, beware). People have finetuned language models on many types of texts.\n",
    "2. Can you think of a way to introduce computational thinking into this? Ideas:\n",
    "  - First, you could explore ways of making things look nicer? Instead of just having a list of objects? You could write a nice print function that knows exactly how to take the model output and print it in a nice way. The specialised Python package with many text functionalities is [textwrap](https://docs.python.org/3/library/textwrap.html) (see also [here](https://www.geeksforgeeks.org/textwrap-text-wrapping-filling-python/);\n",
    "  - Can you think of ways to construct a writing **loop**? By that, I mean:  \n",
    "    a. Prepare prompt  \n",
    "    b. Generate one or more strands of text  \n",
    "    c. Select text from strands, go back to a.  \n",
    "    This could simply mean writing a system of helper functions and classes to assist you in the writing...\n",
    "  - One could imagine all sorts of strange ways to work with text, from programmatically chunking the generated text and scrambling it before using it again as a prompt, to explore what the model does if you use unreasonable parameters (e.g. a very high or low `temperature`).\n",
    "  - Also, can you think of ways to work with various strands of text (Taking advantage of the fact that a model can generate in parallel)?\n",
    "\n",
    "3. Something that has already been the subject of a lot of debate and controversy, is the exploration of the *biases* of the models (and there are tons!). GPT-2 was trained mostly on Internet text, top-ranked reddit posts, etc. (see [this open-source replication](https://github.com/jcpeterson/openwebtext)). Unsurprisingly, the topics and points of view reflect that corner of human activities..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
