{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the GAN results\n",
    "\n",
    "Let's visualize some random results from the GAN. To do so we just need to load a generator model and feed it with random Gaussian noise of the approriate size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results in a notebook\n",
    "\n",
    "The variable `LATENT_DIM` defines the dimension of your [\"latent vector\"](https://medium.com/@jain.yasha/gan-latent-space-1b32cd34cfda). If you changed the same variable in the training notebook, you will have to change it here as well.\n",
    "\n",
    "The third, `iter` defines the epoch for which you want to load a model. You can examine the directory and the example images for each epoch, to choose which epoch you want to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 100 # The size of the latent space/input vector\n",
    "\n",
    "N_CHANNELS = 1 # 3 for colour\n",
    "IMAGE_SHAPE = (N_CHANNELS,64,64) # C, H, W\n",
    "\n",
    "G_DIM = 64\n",
    "D_DIM = 64\n",
    "\n",
    "# fixed directory structure -------------\n",
    "DATASETS_DIR = pathlib.Path(\"datasets\")\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "GENERATED_DIR = pathlib.Path(\"generated\")\n",
    "GENERATED_DIR.mkdir(exist_ok=True)\n",
    "# ----------------------------------------\n",
    "\n",
    "MODEL_NAME = \"dcgan_mnist\" # change accordingly\n",
    "\n",
    "DCGAN_DIR = MODELS_DIR / MODEL_NAME\n",
    "\n",
    "ITERS = 936 # change if needed\n",
    "GENERATOR_NAME = f\"{MODEL_NAME}_g.iter_{ITERS:04d}.pt\"\n",
    "\n",
    "# generated images\n",
    "GENERATED_DIR = GENERATED_DIR / f\"{MODEL_NAME}_images\"\n",
    "GENERATED_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must redefine the net, then load the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, n_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            #                  input, output, kernel, stride, padding\n",
    "            nn.ConvTranspose2d(latent_dim, output_dim * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(G_DIM * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (G_DIM*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(output_dim * 8, output_dim * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(output_dim * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (output_dim*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(output_dim * 4, output_dim * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(output_dim * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (output_dim*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(output_dim * 2, output_dim, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (output_dim) x 32 x 32\n",
    "            nn.ConvTranspose2d(output_dim, n_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (n_channels) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "G = Generator(\n",
    "    latent_dim=LATENT_DIM,\n",
    "    output_dim=G_DIM,\n",
    "    n_channels=N_CHANNELS\n",
    ").to(device)\n",
    "\n",
    "# reloading\n",
    "G.load_state_dict(\n",
    "    torch.load(\n",
    "        DCGAN_DIR / GENERATOR_NAME,\n",
    "        map_location=torch.device(device)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(G)\n",
    "print()\n",
    "print(f\"Our model has {sum(p.numel() for p in G.parameters()):,} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def denorm(x):\n",
    "    \"\"\"Denormalize the outputs from [-1, 1] to [0,1] (generator with 'tanh' activation)\"\"\"\n",
    "    return (x * 0.5) + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 64\n",
    "fixed_noise = torch.randn(N_IMAGES, LATENT_DIM, 1, 1, device=device)\n",
    "\n",
    "def make_grid(noise, iters=0, figsize=(8,8), show=True, save=False):\n",
    "    with torch.no_grad():\n",
    "        output = G(noise).cpu().detach()    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(\n",
    "        TF.to_pil_image(\n",
    "            tv.utils.make_grid(output, padding=2, normalize=True).cpu()\n",
    "        )\n",
    "    )\n",
    "    if save:\n",
    "        plt.savefig(GENERATED_DIR / f\"single_image.iter_{iters}_{i:04d}.png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "make_grid(fixed_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 3\n",
    "fixed_noise = torch.randn(N_IMAGES, LATENT_DIM, 1, 1, device=device)\n",
    "\n",
    "def make_images(noise, iters=0, figsize=(6,6), show=True, save=False):\n",
    "    with torch.no_grad():\n",
    "        output = G(noise).cpu().detach()\n",
    "    for i, o in enumerate(output):\n",
    "        img = TF.to_pil_image(denorm(o))\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        if save:\n",
    "            plt.savefig(GENERATED_DIR / f\"single_image.iter_{iters}_{i:04d}.png\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "make_images(fixed_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk\n",
    "\n",
    "We can use a loop to gradually add some random noise to our latent vector, effectively 'moving' (blindly, chaotically) in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = torch.randn(1, 100,1,1)\n",
    "noise = torch.randn(seed.size()) * .2\n",
    "noise = torch.rand(seed.size()) * .2\n",
    "noise.min(), noise.max(), noise.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_random_walk(generator, latent_dim, n=64, noise_mode=\"normal\"):\n",
    "    seed = torch.randn(1, latent_dim,1,1)\n",
    "    random_latent_vectors = [seed]\n",
    "    for t in range(1, n):\n",
    "        if noise_mode == \"normal\":\n",
    "            # `randn` yields normally distributed numbers (mean 0, std 1)\n",
    "            # -> for an std of .2, we multiply by it (can be tweaked!\n",
    "            noise = torch.randn(seed.size()) * .2\n",
    "        if noise_mode == \"uniform\":\n",
    "            # Uniform Noise, between 0 and 1, we shift that by .5:\n",
    "            # try tweaking the min/max values!\n",
    "            noise =  torch.rand(seed.size()) * .2 - .5\n",
    "        # increment our vector\n",
    "        random_latent_vectors.append(random_latent_vectors[-1] + noise)\n",
    "     # stack the tensors along the batch dim (0) and move to device\n",
    "    return torch.cat(random_latent_vectors, dim=0).to(device)\n",
    "\n",
    "make_grid(generate_random_walk(G, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_grid(generate_random_walk(G, 100, noise_mode=\"uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_images(generate_random_walk(G, 100, n=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Interpolating in latent space\n",
    "\n",
    "We can interpolate between one point in the latent space (the variable `a`) and another point (the variable `b`) to produce a smooth transition between images generated by the GAN along the latent space. It is recommended to use a geod \"spherical linear interpolation\", which effectively describes a [\"geodesic\"](https://en.wikipedia.org/wiki/Geodesic) ([mini-vid](https://www.youtube.com/watch?v=KsdIuVByfMc)). We use spherical interpolation because the multivariate Gaussian used as an input to the GAN generator can be approximated by a hypersphere (a sphere in high dimensions).\n",
    "\n",
    "See [this discussion](https://github.com/soumith/dcgan.torch/issues/14) and [this post](https://machinelearningmastery.com/how-to-interpolate-and-perform-vector-arithmetic-with-faces-using-a-generative-adversarial-network/) for technical details and to see where the interpolation code comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def lerp(t, a, b):\n",
    "    return a + t*(b - a)\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    omega = np.arccos(np.clip(np.dot(low/norm(low), high/norm(high)), -1.0, 1.0))\n",
    "    so = np.sin(omega)\n",
    "    if so == 0:\n",
    "        # L'Hopital's rule/LERP\n",
    "        return (1.0-val) * low + val * high\n",
    "    return np.sin((1.0-val)*omega) / so * low + np.sin(val*omega) / so * high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed1 = torch.randn(1, 100,1,1)\n",
    "seed2 = torch.randn(1, 100,1,1)\n",
    "b = seed1 @ seed2\n",
    "# c = torch.sum(seed1 * seed2, dim=-1)\n",
    "d = (seed1 @ seed2.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slerp(val, low, high):\n",
    "\n",
    "    # Compute the cosine of the angle between the vectors and clip \n",
    "    # it to avoid out-of-bounds errors\n",
    "    omega = torch.acos(torch.clamp(low @ high, -1.0, 1.0))\n",
    "    so = torch.sin(omega)\n",
    "\n",
    "    return torch.where(\n",
    "        so == 0,\n",
    "        # If sin(omega) is 0, use LERP (linear interpolation)\n",
    "        (1.0 - val) * low + val * high,\n",
    "        # Otherwise perform spherical interpolation (SLERP)\n",
    "        (torch.sin((1.0 - val) * omega) / so) * low + (torch.sin(val * omega) / so) * high\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interpolated(generator, latent_dim, n=64):\n",
    "    # here we create to random vectors: one other way might be \n",
    "    # to create one, then add a tiny bit of noise to it, then\n",
    "    # interpolate between the two?\n",
    "    seed1 = torch.randn(1, latent_dim,1,1)\n",
    "    seed2 = torch.randn(1, latent_dim,1,1)\n",
    "    random_latent_vectors = []\n",
    "    for t in np.linspace(0, 1, n):\n",
    "        random_latent_vectors.append(slerp(t,seed1, seed2))\n",
    "     # stack the tensors along the batch dim (0) and move to device\n",
    "    return torch.cat(random_latent_vectors, dim=0).to(device)\n",
    "\n",
    "make_grid(generate_interpolated(G, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_images(generate_interpolated(G, 100, n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the tools from the training notebook, it is possible to create videos using these!\n",
    "\n",
    "There are other, advanced techniques that people have explored here's, a tutorial with a few ideas (in TensorFlow): [Generate Artificial Faces with CelebA Progressive GAN Model](https://www.tensorflow.org/hub/tutorials/tf_hub_generative_image_module)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
