{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6un0U90ipCJy",
    "tags": []
   },
   "source": [
    "# Training a DCGAN I\n",
    "## (Deep Convolutional Generative Adversarial Networks)\n",
    "\n",
    "This is adapted from [this PyTorch example](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html), see also [the same as source code](https://github.com/pytorch/examples/blob/main/dcgan/main.py).\n",
    "\n",
    "Some elements were retained from a previous version, inspired by [this TensorFlow tutorial](https://www.tensorflow.org/tutorials/generative/dcgan) (you can also look at [this Chollet notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter12_part05_gans.ipynb), itself a port of [this Keras tutorial](https://keras.io/examples/generative/dcgan_overriding_train_step/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db0CS0Qr2gvI",
    "tags": []
   },
   "source": [
    "#### Install Imageio (to generate GIFs at the end)\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge imageio # locally (ships with Colab)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfWqbE7522pC"
   },
   "source": [
    "#### reminder: Colab code to mount your drive\n",
    "\n",
    "```python\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')  # 'My Drive' is the default name of Google Drives\n",
    "    os.chdir('drive/My Drive/IS53055B-DMLCP/DMLCP') # change to your favourite dir\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfIk2es3hJEd",
    "outputId": "2725a2b4-331f-4551-ab04-2afde2a54793",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Video\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8ET96hX2gvJ"
   },
   "source": [
    "## Prepare our Dataset\n",
    "\n",
    "### Train on [MNIST](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drvRYcZp5xpw"
   },
   "outputs": [],
   "source": [
    "# # Model / data parameters\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "LATENT_DIM = 100 # The size of the latent space/input vector\n",
    "\n",
    "N_CHANNELS = 3 # 3 for colour\n",
    "IMAGE_SHAPE = (N_CHANNELS,64,64) # C, H, W\n",
    "\n",
    "G_DIM = 64\n",
    "D_DIM = 64\n",
    "\n",
    "# fixed directory structure -------------\n",
    "DATASETS_DIR = pathlib.Path(\"datasets\")\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "GENERATED_DIR = pathlib.Path(\"generated\")\n",
    "GENERATED_DIR.mkdir(exist_ok=True)\n",
    "# ----------------------------------------\n",
    "\n",
    "MODEL_NAME = \"dcgan_mnist\" # change accordingly\n",
    "\n",
    "DCGAN_DIR = MODELS_DIR / MODEL_NAME\n",
    "\n",
    "GENERATOR_NAME = f\"{MODEL_NAME}_g\"\n",
    "DISCRIMINATOR_NAME = f\"{MODEL_NAME}_d\"\n",
    "\n",
    "# generated images\n",
    "DCGAN_GEN_DIR = MODELS_DIR / f\"{MODEL_NAME}_images\"\n",
    "DCGAN_GEN_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kJpoPv22gvJ"
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def denorm(x):\n",
    "    \"\"\"Denormalize the outputs from [-1, 1] to [0,1] (generator with 'tanh' activation)\"\"\"\n",
    "    return (x * 0.5) + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2nznCgb2gvJ",
    "outputId": "7be276e7-7fcc-43dc-93e9-006ef9af5c26"
   },
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True), # from [0,255] to [0,1]\n",
    "    v2.Resize(IMAGE_SHAPE[1]),\n",
    "    v2.CenterCrop(IMAGE_SHAPE[1]),\n",
    "    v2.Normalize(mean=[0.5]*N_CHANNELS, std=[0.5]*N_CHANNELS) # (x - mean)/std\n",
    "])\n",
    "\n",
    "train_data = tv.datasets.FashionMNIST(\n",
    "    root=DATASETS_DIR,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms,\n",
    ")\n",
    "\n",
    "for t in train_data:\n",
    "    print(t[0].shape, t[0].dtype, t[0].min(), t[0].max())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ynu-jsj12gvJ"
   },
   "source": [
    "Create `DataLoader`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvXcM--G2gvJ",
    "outputId": "ebde79dc-d20a-415a-c15e-bfc03e5ecf02"
   },
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape} {X.dtype}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Q4HE2vRbY29"
   },
   "source": [
    "### Inspect our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "s1vHScoj2gvJ",
    "outputId": "4a47c2f5-da58-4203-81f3-f66226a13bf9"
   },
   "outputs": [],
   "source": [
    "for i, t in enumerate(train_dataloader):\n",
    "    x, y = t\n",
    "    a = denorm(x[0])\n",
    "    img = TF.to_pil_image(a)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hP7rpNredvG"
   },
   "source": [
    "[`tv.utils.make_grid`](https://pytorch.org/vision/main/generated/torchvision.utils.make_grid.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "Po1odpDjYnU7",
    "outputId": "a816f66d-d30c-4a8f-c6f8-57430cb1fcd9"
   },
   "outputs": [],
   "source": [
    "for i, t in enumerate(train_dataloader):\n",
    "    x, y = t\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Training Images\")\n",
    "    plt.imshow(TF.to_pil_image(tv.utils.make_grid(x[:64], padding=2, normalize=True).detach().cpu()))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1L7R991L2gvK",
    "outputId": "8d8c35af-3700-4aa1-f9bd-fb83697dd39b"
   },
   "outputs": [],
   "source": [
    "ds_len = len(train_dataloader)\n",
    "print(f\"{ds_len * BATCH_SIZE} samples in {ds_len} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Create the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgXEj6FRDtoF"
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on ``G`` and ``D``\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tEyxE-GMC48"
   },
   "source": [
    "### The Generator\n",
    "\n",
    "Source [here](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#generator). The generator uses `nn.ConvTranspose2d` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bpTcDqoLWjY",
    "outputId": "a0a41dae-c7ba-4249-d3c1-c65469bf2e67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, n_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            #                  input, output, kernel, stride, padding\n",
    "            nn.ConvTranspose2d(latent_dim, output_dim * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(output_dim * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (output_dim*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(output_dim * 8, output_dim * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(output_dim * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (output_dim*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(output_dim * 4, output_dim * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(output_dim * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (output_dim*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(output_dim * 2, output_dim, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (output_dim) x 32 x 32\n",
    "            nn.ConvTranspose2d(output_dim, n_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (n_channels) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "G = Generator(\n",
    "    latent_dim=LATENT_DIM,\n",
    "    output_dim=G_DIM,\n",
    "    n_channels=N_CHANNELS\n",
    ").to(device)\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize all weights\n",
    "#  to ``mean=0``, ``stdev=0.02``.\n",
    "G.apply(weights_init)\n",
    "\n",
    "# reloading\n",
    "RELOAD_G_PATH = \"\" # change to existing path\n",
    "if RELOAD_G_PATH != \"\":\n",
    "    G.load_state_dict(torch.load(RELOAD_G_PATH))\n",
    "\n",
    "print(G)\n",
    "print()\n",
    "print(f\"Our model has {sum(p.numel() for p in G.parameters()):,} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YueDSoGx2gvK"
   },
   "source": [
    "Let&rsquo;s see it&rsquo;s output before training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "gl7jcC7TdPTG",
    "outputId": "4e6e2904-99c2-4f81-fcf7-e18a73ab488a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise = torch.randn([1, LATENT_DIM, 1, 1]).to(device) # shape required by the convolution\n",
    "print(noise.size())\n",
    "\n",
    "with torch.no_grad():\n",
    "    generator_output = G(noise)\n",
    "    print(generator_output.shape, generator_output.min(), generator_output.max())\n",
    "    # move to cpu, denormalize, turn into PIL\n",
    "    img = TF.to_pil_image(\n",
    "        denorm(generator_output[0].detach().cpu())\n",
    "    )\n",
    "\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0IKnaCtg6WE"
   },
   "source": [
    "### The Discriminator\n",
    "\n",
    "The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dw2tPLmk2pEP",
    "outputId": "bb4997bc-5e72-4621-bffd-43f97c05ead6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_shape, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (n_channels) x 64 x 64\n",
    "            #         input, output, kernel, stride, padding\n",
    "            nn.Conv2d(image_shape[0], input_dim, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (input_dim) x 32 x 32\n",
    "            nn.Conv2d(input_dim, input_dim * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(input_dim * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (input_dim*2) x 16 x 16\n",
    "            nn.Conv2d(input_dim * 2, input_dim * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(input_dim * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (input_dim*4) x 8 x 8\n",
    "            nn.Conv2d(input_dim * 4, input_dim * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(input_dim * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (input_dim*8) x 4 x 4\n",
    "            nn.Conv2d(input_dim * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "D = Discriminator(\n",
    "    image_shape=IMAGE_SHAPE,\n",
    "    input_dim=64,\n",
    ").to(device)\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize all weights\n",
    "#  to ``mean=0``, ``stdev=0.02``.\n",
    "D.apply(weights_init)\n",
    "\n",
    "# reloading\n",
    "RELOAD_D_PATH = \"\" # change to existing path\n",
    "if RELOAD_D_PATH != \"\":\n",
    "    D.load_state_dict(torch.load(RELOAD_D_PATH))\n",
    "\n",
    "print(D)\n",
    "print()\n",
    "print(f\"Our model has {sum(p.numel() for p in D.parameters()):,} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhPneagzCaQv"
   },
   "source": [
    "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDkA05NE6QMs",
    "outputId": "1f0c02a5-5604-4ca3-8e57-6bb918dc1adf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decision = D(generator_output).detach().cpu()\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the loss and optimizers\n",
    "\n",
    "Define loss functions and optimizers for both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psQfmXxYKU3X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the ``BCELoss`` function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "LEARNING_RATE = 0.0002\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerG = torch.optim.Adam(\n",
    "    G.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizerD = torch.optim.Adam(\n",
    "    D.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBIDnvwKU_Ui"
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 64\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(N_IMAGES, LATENT_DIM, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9ID5DnSULQt"
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "EPOCHS = 5\n",
    "PRINT_EVERY = 50           # (Batch) print stats\n",
    "SHOW_EVERY = 500           # (Batch) show grid\n",
    "SAVE_GRID_EVERY = 50       # (Batch) save grid (for GIF, see below)\n",
    "SAVE_MODEL_EVERY = EPOCHS  # (Epoch) save model (skips epoch 0 unless it is 1: 'every epoch')\n",
    "\n",
    "tot = len(train_dataloader) # for print formatting\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(EPOCHS):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "\n",
    "        real, _ = data         # we don't use the labels\n",
    "        real = real.to(device) # move to accelerator\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # -----------------------------------------------------------\n",
    "        ## Train with all-real batch\n",
    "\n",
    "        D.zero_grad()\n",
    "        label = torch.full((BATCH_SIZE,), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = D(real).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item() # D accuracy on real\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(BATCH_SIZE, LATENT_DIM, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = G(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = D(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item() # D accuracy on fake 1\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        # -------------------------------------------\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        # -------------------------------------------\n",
    "\n",
    "        G.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = D(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item() # D accuracy on fake 2\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % PRINT_EVERY == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1:{len(str(EPOCHS))}}/{EPOCHS} | Batch: {i:{len(str(tot))}}/{tot} | \"\n",
    "                + f\"Losses, D: {errD.item():.5f}, G: {errG.item():.5f} | \"\n",
    "                + f\"Accs, D(x): {D_x:.5f}, D(G(z)): {D_G_z1:.5f} / {D_G_z2:.5f}\"\n",
    "            )\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if iters % SAVE_GRID_EVERY == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = G(fixed_noise).detach().cpu()\n",
    "\n",
    "            fake_img = TF.to_pil_image(tv.utils.make_grid(fake, padding=2, normalize=True))\n",
    "            img_list.append(fake_img)\n",
    "\n",
    "            # Save our images\n",
    "            plt.figure(figsize=(12,12))\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(fake_img)\n",
    "            plt.savefig(DCGAN_GEN_DIR / f\"grid_iter_{iters:04d}.png\")\n",
    "            plt.close()\n",
    "\n",
    "        # Show grid\n",
    "        if iters % SHOW_EVERY == 0:\n",
    "            plt.figure(figsize=(12,12))\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(fake_img)\n",
    "            plt.show()\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    # Save our model (if every epoch, we save even at the first epoch, otherwise no)\n",
    "    if (epoch > 0 or SAVE_MODEL_EVERY == 1) and (epoch + 1) % SAVE_MODEL_EVERY == 0:\n",
    "        print(f\"Saving Generator and Discriminator to {DCGAN_DIR}\")\n",
    "        torch.save(G.state_dict(), DCGAN_DIR / f\"{GENERATOR_NAME}.iter_{iters:04d}.pt\")\n",
    "        torch.save(D.state_dict(), DCGAN_DIR / f\"{DISCRIMINATOR_NAME}.iter_{iters:04d}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "MZGQ2FRgxQFZ",
    "outputId": "a5cb31c4-eed8-4beb-be7c-90c04e8e03e4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cl9BcmJqovyA"
   },
   "source": [
    "## Generate video from images (using matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "id": "F5u1jJAqxS6S",
    "outputId": "cca48a38-5318-4529-ef97-479183c9628a"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(i, animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(\n",
    "    fig, ims, interval=100, # determines the speed\n",
    "    repeat_delay=1000, blit=True\n",
    ")\n",
    "plt.close(fig) # Close the figure to avoid displaying a static image (ChatGPT 4o)\n",
    "\n",
    "# .gif also possible\n",
    "ani.save(DCGAN_GEN_DIR / \"grid.mp4\", writer=\"ffmpeg\", dpi=80)\n",
    "\n",
    "Video(DCGAN_GEN_DIR / \"grid.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTXFlKdlpZv6"
   },
   "source": [
    "### Download from Colab (using Unix utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5c4obQfpe8D"
   },
   "source": [
    "```bash\n",
    "!rm -rf models/.ipynb_checkpoints # remove ipynb fluff\n",
    "!zip -r dcgan_mnist.zip models    # zip model folder\n",
    "```\n",
    "Then go to the left-hand side bar, click on the folder icon, and use the three dots on the right of the zip file to download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4M_vIbUi7c0"
   },
   "source": [
    "## Extra: Create a GIF from files (using imageio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ha9A7J-o2xlx"
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import base64\n",
    "import textwrap\n",
    "import mimetypes\n",
    "import imageio as iio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfO5wCdclHGL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(iter_no):\n",
    "    return PIL.Image.open(\n",
    "        DCGAN_GEN_DIR / f\"grid_iter_{iter_no:04d}.png\"\n",
    "    ).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5x3q9_Oe5q0A",
    "outputId": "afcb3a0e-7d0e-4c7d-b330-5e5f40815d6c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_image(920) # check the directory for the correct number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqxR7q9R2gvN"
   },
   "outputs": [],
   "source": [
    "ANIM_PATH = DCGAN_GEN_DIR / 'grid_redux.gif'\n",
    "\n",
    "# adapting the the tutorial version to v3 + looping the gif (thanks ChatGPT)\n",
    "with iio.get_writer(ANIM_PATH, mode='I', loop=0) as writer:\n",
    "    # IDEA: here we go from start to finish, and we loop...\n",
    "    #       we could add those images a second time but\n",
    "    #       *backward*, so that the gif will return to the\n",
    "    for f in sorted(DCGAN_GEN_DIR.glob('grid*.png')):\n",
    "        image = iio.v3.imread(f)\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZBwyU6t2Wf3g",
    "outputId": "154cfb0b-a358-4fef-8012-a1383e484515",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adapted from here: https://github.com/tensorflow/docs/blob/master/tools/tensorflow_docs/vis/embed.py\n",
    "\n",
    "def embed_data(mime, data):\n",
    "    \"\"\"Embeds data as an html tag with a data-url.\"\"\"\n",
    "    b64 = base64.b64encode(data).decode()\n",
    "    if mime.startswith('image'):\n",
    "        tag = f'<img src=\"data:{mime};base64,{b64}\"/>'\n",
    "    elif mime.startswith('video'):\n",
    "        tag = textwrap.dedent(f\"\"\"\n",
    "            <video width=\"640\" height=\"480\" controls>\n",
    "              <source src=\"data:{mime};base64,{b64}\" type=\"video/mp4\">\n",
    "              Your browser does not support the video tag.\n",
    "            </video>\n",
    "            \"\"\")\n",
    "    else:\n",
    "        raise ValueError('Images and Video only.')\n",
    "    return HTML(tag)\n",
    "\n",
    "def embed_file(path):\n",
    "    \"\"\"Embeds a file in the notebook as an html tag with a data-url.\"\"\"\n",
    "    path = pathlib.Path(path)\n",
    "    mime, unused_encoding = mimetypes.guess_type(str(path))\n",
    "    data = path.read_bytes()\n",
    "    return embed_data(mime, data)\n",
    "\n",
    "embed_file(ANIM_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWVw0BSx2gvN"
   },
   "source": [
    "---\n",
    "\n",
    "## Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pgp5IHhYvPwQ"
   },
   "source": [
    "### Use your model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_0SsTXAvX5T"
   },
   "source": [
    "\n",
    "Use your saved model in the [05_dcgan_visualizing_result.ipynb](https://github.com/jchwenger/DMLCP/blob/main/python/05_dcgan_visualizing_results.ipynb) notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTXOUOX8un3a"
   },
   "source": [
    "### Work with the animation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHKJbv4WvY8R"
   },
   "source": [
    "\n",
    "First of all, note that we create the latent noise tensor at the beginning of training: if we created one inside the loop, things would look much more chaotic!\n",
    "\n",
    "You might want to have a different grid, or just one image. Using this function, you could generate one image at a time (modify the size in `figsize`):\n",
    "\n",
    "```python\n",
    "N_IMAGES = 3\n",
    "fixed_noise = torch.randn(N_IMAGES, LATENT_DIM, 1, 1, device=device)\n",
    "\n",
    "def save_images(noise, iters=0, save=True, show=True):\n",
    "    with torch.no_grad():\n",
    "        output = G(noise).cpu().detach()\n",
    "    for i, o in enumerate(output):\n",
    "        img = TF.to_pil_image(denorm(o))\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        if save:\n",
    "            plt.savefig(DCGAN_GEN_DIR / f\"single_image.iter_{iters}_{i:04d}.png\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "save_images(fixed_noise)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-we1hSd8ufq1"
   },
   "source": [
    "### Train on a different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy31mdpy53D1"
   },
   "source": [
    "The code above also works with other datasets! You can for instance replace `MNIST` in `tv.datasets.MNIST` by:\n",
    "  - [`FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html?highlight=fashionmnist#torchvision.datasets.FashionMNIST)\n",
    "  - [`CIFAR10`](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10)\n",
    "  - [`LSUN`](https://pytorch.org/vision/stable/generated/torchvision.datasets.LSUN.html#torchvision.datasets.LSUN) (requires `pip install lmdb`)\n",
    "  - [`EMNIST`](https://pytorch.org/vision/stable/generated/torchvision.datasets.EMNIST.html#torchvision.datasets.EMNIST)\n",
    "\n",
    "  - Or your own data:\n",
    "\n",
    "  ```python\n",
    "  train_data = tv.datasets.ImageFolder(\n",
    "      root=DATASETS_DIR,\n",
    "      transform=transforms,\n",
    "  )\n",
    "  ```\n",
    "\n",
    "  Fun fact: there are [even more MNIST-like datasets](https://www.simonwenkel.com/lists/datasets/list-of-mnist-like-datasets.html). (Generating an MNIST-like dataset could be done using for instance [p5js-ccapture](https://github.com/jchwenger/p5js-ccapture).)\n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kI19TJoDqt9i"
   },
   "source": [
    "#### Train on CelebA (celeb faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RioTsqLvtQ7"
   },
   "source": [
    "Special steps are required to download the [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDgRkhHHqxYU"
   },
   "source": [
    "##### 1. Colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vzaYHhqvu35"
   },
   "source": [
    "\n",
    "My recommendation is the following:\n",
    "\n",
    "1. Download the dataset from the Google Drive of the authors **once**, and upload the `img_align_celeba.zip` file (1.4 GB!) to your drive.  \n",
    "    1. Manually: [here](https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684)  \n",
    "    2. Using `gdown`: `!gdown 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684`\n",
    "\n",
    "The code snippet here will download the dataset using Python:\n",
    "```python\n",
    "import gdown\n",
    "from zipfile import ZipFile\n",
    "\n",
    "CELEBA_DIR = DATASETS_DIR / \"dcgan_celeba\"\n",
    "EXTRACTED_DIR = os.path.join(CELEBA_DIR, \"img_align_celeba\")\n",
    "\n",
    "LE_ID = '1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684' # add your ID here\n",
    "\n",
    "if not os.path.isdir(EXTRACTED_DIR):\n",
    "    if LE_ID is None:\n",
    "        print(\"Variable `LE_ID` is None: upload the Celeba dataset to your drive, retrieve its id, and add it to `LE_ID`!\")\n",
    "    else:\n",
    "        print(\"Downloading Celeba dataset\")\n",
    "        os.makedirs(CELEBA_DIR, exist_ok=True)\n",
    "\n",
    "        fname = \"datasets/dcgan_celeba/data.zip\"\n",
    "        url = f\"https://drive.google.com/uc?id={LE_ID}\"\n",
    "        gdown.download(url, fname, quiet=False)\n",
    "\n",
    "        print(\"Unzipping\")\n",
    "        with ZipFile(\"datasets/dcgan_celeba/data.zip\", \"r\") as zipobj:\n",
    "            zipobj.extractall(\"datasets/dcgan_celeba\")\n",
    "else:\n",
    "    print(\"CelebA directory exists\")\n",
    "\n",
    "# all the images will be of the classd 'img_align_celeba',\n",
    "# the only folder in there, but we don't care\n",
    "train_data = tv.datasets.ImageFolder(\n",
    "   root=CELEBA_DIR,\n",
    "   transform=transforms,\n",
    ")\n",
    "```\n",
    "\n",
    "Note that the bash commands below do the same as the Python code above:\n",
    "\n",
    "```bash\n",
    "!mkdir -p datasets/dcgan_celeba\n",
    "!gdown 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684 -O datasets/dcgan_celeba/img_align_celeba.zip\n",
    "!unzip -qq datasets/dcgan_celeba/img_align_celeba.zip -d datasets/dcgan_celeba\n",
    "```\n",
    "\n",
    "Perform these steps first, *then* connect to your drive and switch directories (if you want to save your model and generated images in your drive, otherwise no need)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2VDKoN6vv4M"
   },
   "source": [
    "##### 2. Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_WHXGLcvwjI"
   },
   "source": [
    "\n",
    "Perform the steps to download the data once and unzip it so your directory looks like `DMLAP/python/datasets/dcgan_celeba/img_align_celeba` (using the lines above or the cell below).\n",
    "\n",
    "To install [gdown](https://pypi.org/project/gdown/): `conda install -c conda-forge gdown`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08Vgn6rezKKO"
   },
   "source": [
    "---\n",
    "\n",
    "## More thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ21Qy5CzTsS"
   },
   "source": [
    "The work with generative models that can be done here broadly falls into three main directions:\n",
    "- *Freeze* the network, work on the dataset:\n",
    "  - In this direction, most of your work is to gather datasets, and improve the ease of use. Are you able to develop a suite of tools that would allow you to handle datasets more easily? (In this case, the images are already cropped and the same size, which already takes some work! It would be nice to integrate tools that allow you to make this part of the work more streamlined: put any images in a folder, and a Python script crops them, etc.)? It might be worth looking into [data augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation) (inject randomness into your image dataset, [this tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix) uses that).\n",
    "  - It would be interesting to train GANs on generative images! You might end up with really distorted versions of what you started with.\n",
    "  - It's likely that people have trained GANs on spectrograms, as we see now with diffusion, but it might be a real fun thing to try?\n",
    "  - The image used for the week on text is a [book project by Allisson Parrish](https://www.aleator.press/releases/wendit-tnce-inf) that uses GANs to generate images of (unreadable) poems!\n",
    "  - Also, people have created loops where they train GANs on their own outputs, which creates distortions that may be worth exploring.\n",
    "- *Freeze* the dataset, work on the network:\n",
    "  - Maybe there's one dataset that's really your focus, or you're happy to work with established material, or the whole data processing feels boring? You might then want to look into fiddling with the model, and gather tricks (for instance: do you see an improvement if you normalise your images to be between [0,1] instead of [-1,1], like here (your Generator will have to have a `sigmoid` rather than a `tanh` as its last layer)? Then of course there's the network themselves, where all sorts of parameters can be tweaked, from the number of layers, to the strides of the convolution...\n",
    "  - **Note:** experimenting at a technical level with GANs (like with other things) can be a confusing rabbit hole. My recommendations are: make sure you have stable resources (e.g. you own a GPU or pay for Colab Pro), and try and make your net/dataset/experiments *as small/easy as possible*, so you can make a lot of them, get an inuition of what works and what doesn't. Perfect results really aren't the goal here, and it's never good for your momentum to have to wait hours or days before training finishes!\n",
    "  - How do you document this process of experimentation? You would probably need to save the various parameters of your experimentation (for yourself and, perhaps, the viewer), and associate that with some images generated at this point.\n",
    "- *Freeze* both network and dataset, and try to use the network, or its output, in unexpected ways: one could imagine just training this network, or using a top-level StyleGAN (see below), and using the resulting images in some way, as material for something else?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQBayvtrzaCB"
   },
   "source": [
    "### The State of the Art\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fowV6D-2zbDQ"
   },
   "source": [
    "\n",
    "The field has now moved away from GANs, as Diffusion has gained in popularity. The best results have probably been achieved by [Nvidia's StyleGan 3](https://nvlabs.github.io/stylegan3/) ([repo](https://github.com/NVlabs/stylegan3)) (both written in PyTorch). Check the [StyleGAN 3 notebook](10_models_1_stylegan3.ipynb) to check it out (on Colab!).\n",
    "\n",
    "Another interesting option to look into is lucidrains' [Lightweight GAN](https://github.com/lucidrains/lightweight-gan) implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tp3oqxqUzcCk"
   },
   "source": [
    "\n",
    "### Zoos: list of all GAN variants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxdpAIYRzdMk"
   },
   "source": [
    "\n",
    "When it comes to GANs, just like Diffusion now, the explosion has been so enormous it is rather difficult (impossible?) to keep up:\n",
    "\n",
    "- [Avinash Hindupur, \"The GAN Zoo\"](https://github.com/hindupuravinash/the-gan-zoo)\n",
    "- [Jihye Back, \"GAN-Zoos\"](https://happy-jihye.github.io/gan/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXE4uyQRzgHR"
   },
   "source": [
    "### Notes / Tricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7i145KBzj3j"
   },
   "source": [
    "\n",
    "More resources worth checking: [Soumith Chintala, \"How to Train a GAN? Tips and tricks to make GANs work\"](https://github.com/soumith/ganhacks) (and [video](https://www.youtube.com/watch?v=X1mUN6dD8uE), as well as [Goodfellow's workshop](https://www.youtube.com/watch?v=HGYYEUSm-0Q)). This is summarised [in this part of a long course](https://www.youtube.com/watch?v=_cUdjPdbldQ&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&index=153). To go deeper still, there's [this paper](https://arxiv.org/abs/1606.03498), and a [GAN guide](https://github.com/garridoq/gan-guide), and the [Art using GANs](https://github.com/Kaustubh1Verma/Art-using-GANs) repo.\n",
    "\n",
    "Here is a summary of some of the tricks Chollet mentions in his book, that are used in this implementation:\n",
    "\n",
    "- Sample from the latent space using a **normal distribution** (Gaussian), not a uniform one;\n",
    "- GANs are likely to get stuck in all sorts of ways (it's an unstable, dynamic equilibrium): we introduce **random noise** to the labels for the discriminator to prevent this (called label smoothing);\n",
    "- Sparse gradients can hinder GAN training, remedy: **strided convolutions** for downsampling instead of max pooling, and the **`LeakyReLU`** instead of `ReLu`;\n",
    "- To avoid checkerboard artifacts caused by unequal coverage of the pixel space in the generator, use a kernel size **divisible by the stride size** with strided `Conv2DTranspose` or `Conv2D`. [In this implementation, we avoid those and use `UpSamling2D` followed by a `Conv2D`].\n",
    "\n",
    "<small>*Deep Learning With Python*, 2<sup>nd</sup> ed., p.404</small>\n",
    "\n",
    "Note also that, as is mentioned by Chintala (see lecture above), the labels for true/fake are reversed from the original formulation (here 0 is true, 1 is fake), that is said to improve stability."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
