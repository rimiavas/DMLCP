{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pix2pix training\n",
    "================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display # we will use this to clear the results\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reminder: Colab code to mount your drive\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')  # 'My Drive' is the default name of Google Drives\n",
    "    os.chdir('drive/My Drive/2023-DMLAP/DMLAP/python') # change to your favourite dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Download a dataset, e.g.:\n",
    "\n",
    "- [Standard pix2pix datasets](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)\n",
    "- [Comic faces](https://www.kaggle.com/datasets/defileroff/comic-faces-paired-synthetic)\n",
    "- [Maps](https://www.kaggle.com/datasets/alincijov/pix2pix-maps)\n",
    "- [Rembrandt](https://www.kaggle.com/datasets/grafstor/rembrandt-pix2pix-dataset)\n",
    "- [Depth](https://www.kaggle.com/datasets/greg115/pix2pix-depth)\n",
    "\n",
    "Or create your own using the notebook.\n",
    "\n",
    "Beware the sizes! Some of them are pretty big.\n",
    "\n",
    "## Load and preprocess dataset\n",
    "\n",
    "\n",
    "Let's first specify the path of our dataset and the desired image size (do not change the latter):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = \"./datasets/edges2comics/\" # Change this for your custom dataset\n",
    "img_channels = 3\n",
    "img_size = 256\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training image in a standard pix2pix dataset consists of one imgage divided into two adjacent **source** and **target** images.\n",
    "The layout of the source and target may vary from training set to trainig set, so we provide a `target_index` flag the determines on which side the target is (`0` if on the left and `1` if on the right). Set this so the examples from the dataset appear with the source image to the left.\n",
    "\n",
    "The following code also **augments** the dataset by applying random uniform scaling (by upscaling and cropping) and random mirroring to the input output pairs. This should lead to a more stable model according to the original pix2pix paper. Finally the images ar normalized to the [-1,1] range as required by our GAN-based model.\n",
    "\n",
    "We will organize the dataset in batches of size `1`, as that is generally suggested for pix2pix models. That means that we will update the weights of the model for each image pair separately.\n",
    "\n",
    "Run the code below and examine the resulting example images. Then set the `target_index` variable to reflect the position of the target image. That is `target_index=0` if the target image is on the left and `target_index=1` if it is on the right.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_index = 1\n",
    "\n",
    "def resize(input_image, target_image, height, width):\n",
    "    input_image = tf.image.resize(\n",
    "        input_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "    )\n",
    "    target_image = tf.image.resize(\n",
    "        target_image, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "    )\n",
    "    return input_image, target_image\n",
    "\n",
    "\n",
    "def random_crop(input_image, target_image):\n",
    "    stacked_image = tf.stack([input_image, target_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(stacked_image, size=[2, 256, 256, 3])\n",
    "    return cropped_image[0], cropped_image[1]\n",
    "\n",
    "@tf.function()\n",
    "def random_jitter(input_image, target_image):\n",
    "    # Resizing to 286x286\n",
    "    input_image, target_image = resize(input_image, target_image, 286, 286)\n",
    "    # Random cropping back to 256x256\n",
    "    \n",
    "    input_image, target_image = random_crop(input_image, target_image)\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # Random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        target_image = tf.image.flip_left_right(target_image)\n",
    "\n",
    "    return input_image, target_image\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Split each image tensor into two tensors:\n",
    "    w = tf.shape(image)[1]\n",
    "    w = w // 2\n",
    "\n",
    "    if target_index == 0:\n",
    "        input_image = image[:, w:, :]\n",
    "        target_image = image[:, :w, :]\n",
    "    else:\n",
    "        target_image = image[:, w:, :]\n",
    "        input_image = image[:, :w, :]\n",
    "        \n",
    "    # Jitter\n",
    "    input_image, target_image = random_jitter(input_image, target_image)\n",
    "    # Normalize\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    target_image = (target_image / 127.5) - 1\n",
    "    return input_image, target_image\n",
    "\n",
    "\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    label_mode=None,\n",
    "    image_size=(img_size * 2, img_size),\n",
    "    batch_size=None\n",
    ")\n",
    "\n",
    "dataset = dataset.map(preprocess_image)\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "for i, (input_img, output_img) in enumerate(dataset):\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"input\")\n",
    "    plt.imshow((input_img.numpy() * 127.5 + 127.5).astype(\"int32\")[0])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"target\")\n",
    "    plt.imshow((output_img.numpy() * 127.5 + 127.5).astype(\"int32\")[0])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build  the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pix2pix model is a conditional generative adversarial network (cGAN). A CGAN\n",
    "is a type of GAN model used for generating new data samples with specific\n",
    "attributes or characteristics. In a CGAN, both the generator and discriminator\n",
    "are *conditioned* on additional information, such as class labels, tags, or\n",
    "other types of metadata. The generator network takes in random noise as well as\n",
    "the conditional information as input and produces a new data sample that matches\n",
    "the desired attributes. The discriminator network, on the other hand, tries to\n",
    "distinguish between the generated samples and real samples based on both their\n",
    "visual appearance and the conditional information. For the case of a pix2pix\n",
    "model the network is conditioned on an image, which should be transformed into\n",
    "an output image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from a DC-GAN, the generator of the pix2pix model is based on the\n",
    "[U-net](https://arxiv.org/abs/1505.04597) architecture. A U-net model is a CNN architecture that is typically used\n",
    "for image segmentation tasks. The name U-net derives from the architecture,\n",
    "which resembles the letter &ldquo;U&rdquo;. It consists of two main parts: an *encoder* and\n",
    "a *decoder*. The encoder part consists of a series of convolutional layers,\n",
    "which reduce the spatial dimensionality of the input image while increasing its\n",
    "depth. This is followed by a bottleneck layer that extracts the most important\n",
    "features from the input image. The decoder part is a &ldquo;mirror image&rdquo; of the\n",
    "encoder. It consists of a series of layers that gradually increase the spatial\n",
    "dimensionality of the output, while decreasing its depth. This is similar to\n",
    "what we have seen in the DC-GAN example, but here we use a &ldquo;transposed\n",
    "convolution&rdquo; layer (`Conv2DTranspose`), while in the DCGAN example we used an\n",
    "upscaling followed by a convolution. The output of each layer in the encoder is\n",
    "also concatenated with the output of another layer in the decoder. This creates\n",
    "&ldquo;skip connections&rdquo; that help preserve spatial information and avoid information\n",
    "loss during the encoding and decoding process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    init = tf.random_normal_initializer(0.0, 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters,\n",
    "            size,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=init,\n",
    "            use_bias=False,\n",
    "        )\n",
    "    )\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "    return result\n",
    "\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    init = tf.random_normal_initializer(0.0, 0.02)\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            filters,\n",
    "            size,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=init,\n",
    "            use_bias=False,\n",
    "        )\n",
    "    )\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "    result.add(tf.keras.layers.ReLU())    \n",
    "    return result\n",
    "\n",
    "\n",
    "def build_generator():\n",
    "    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False), # (batch_size, 128, 128, 64)\n",
    "        downsample(128, 4),                       # (batch_size, 64, 64, 128)\n",
    "        downsample(256, 4),                       # (batch_size, 32, 32, 256)\n",
    "        downsample(512, 4),                       # (batch_size, 16, 16, 512)\n",
    "        downsample(512, 4),                       # (batch_size, 8, 8, 512)\n",
    "        downsample(512, 4),                       # (batch_size, 4, 4, 512)\n",
    "        downsample(512, 4),                       # (batch_size, 2, 2, 512)\n",
    "        downsample(512, 4),                       # (batch_size, 1, 1, 512)\n",
    "    ]\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),     # (batch_size, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),     # (batch_size, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),     # (batch_size, 8, 8, 1024)\n",
    "        upsample(512, 4),                         # (batch_size, 16, 16, 1024)\n",
    "        upsample(256, 4),                         # (batch_size, 32, 32, 512)\n",
    "        upsample(128, 4),                         # (batch_size, 64, 64, 256)\n",
    "        upsample(64, 4),                          # (batch_size, 128, 128, 128)\n",
    "    ]\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "        img_channels, 4,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer=initializer,\n",
    "        activation='tanh'\n",
    "    )                                             # (batch_size, 256, 256, 3)\n",
    "    x = inputs\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    skips = reversed(skips[:-1])\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "    x = last(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "generator = build_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator in the pix2pix model is a convolutional &ldquo;PatchGAN classifier&rdquo;.\n",
    "It tries to classify if each **image patch** if it is real or not real. In the\n",
    "following decoder, each 30 x 30 image patch of the output classifies a 70 x 70\n",
    "portion of the input image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    init = tf.random_normal_initializer(0.0, 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=[256, 256, 3], name=\"input_image\")\n",
    "    tar = tf.keras.layers.Input(shape=[256, 256, 3], name=\"target_image\")\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar])             # (batch_size, 256, 256, channels*2)\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x)                     # (batch_size, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1)                       # (batch_size, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2)                       # (batch_size, 32, 32, 256)\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)      # (batch_size, 34, 34, 256)\n",
    "    conv = tf.keras.layers.Conv2D(\n",
    "        512, 4,\n",
    "        strides=1,\n",
    "        kernel_initializer=init,\n",
    "        use_bias=False\n",
    "    )(zero_pad1)                                            # (batch_size, 31, 31, 512)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (batch_size, 33, 33, 512)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(\n",
    "        1, 4,\n",
    "        strides=1,\n",
    "        kernel_initializer=init\n",
    "    )(zero_pad2)                                            # (batch_size, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some images before training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let&rsquo;s generate some images before training to see what the network will output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar, fname=''):\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    if tar is not None:\n",
    "        display_list = [test_input[0], tar[0], prediction[0]]\n",
    "        title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "    else:\n",
    "        display_list = [test_input[0], prediction[0]]\n",
    "        title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    for i in range(len(title)):\n",
    "        plt.subplot(1, len(title), i+1)\n",
    "        plt.title(title[i])\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "\n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "for example_input, example_target in dataset.take(3):\n",
    "    generate_images(generator, example_input, example_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
    "\n",
    "-   The generator loss is a sigmoid cross-entropy loss of the generated images and an array of ones.\n",
    "-   The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.\n",
    "-   This allows the generated image to become structurally similar to the target image.\n",
    "-   The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper.\n",
    "\n",
    "Feel free to experiment with modifying the value of `LAMBDA` (if you have time to spare:))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LAMBDA = 100\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_fn(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output)) # mean absolute error\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator loss function takes 2 inputs: real images and generated images.\n",
    "\n",
    "-   The `real_loss` is a sigmoid cross-entropy loss of the real images and an array of ones (since these are the real images).\n",
    "-   The `generated_loss` is a sigmoid cross-entropy loss of the generated images and an array of zeros (since these are the fake images).\n",
    "-   The `total_loss` is simply the sum of `real_loss` and `generated_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_fn(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    generated_loss = loss_fn(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop procedes as follows:\n",
    "\n",
    "-   For each example input we generate an output.\n",
    "-   The discriminator receives the input image and the generated image as the first input. The second input is the input image and the target image.\n",
    "-   Next, calculate the generator and the discriminator loss.\n",
    "-   Then, calculate the gradients of the loss with respect to both the generator and the discriminator variables (inputs) and apply those to the optimizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j, (example_input, example_target) in enumerate(dataset.take(3)):\n",
    "    generate_images(\n",
    "        generator,\n",
    "        example_input,\n",
    "        example_target,\n",
    "        fname=os.path.join(generated_dir, f\"e{epoch+1:03}d_generated_image_{j+1}.png\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "save_interval = 1\n",
    "\n",
    "model_dir = \"./models/edges2comics\" # Can change. Model files and resulting images will be saved in this directory\n",
    "os.makedirs(model_dir, exist_ok=True) # this actually creates the directory if it does not exist\n",
    "\n",
    "generated_dir = os.path.join(model_dir , \"generated\")\n",
    "os.makedirs(generated_dir, exist_ok=True)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    input_image, target = batch\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(\n",
    "            disc_generated_output, gen_output, target\n",
    "        )\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "        generator_gradients = gen_tape.gradient(\n",
    "            gen_total_loss, generator.trainable_variables\n",
    "        )\n",
    "        discriminator_gradients = disc_tape.gradient(\n",
    "            disc_loss, discriminator.trainable_variables\n",
    "        )\n",
    "\n",
    "        generator_optimizer.apply_gradients(\n",
    "            zip(generator_gradients, generator.trainable_variables)\n",
    "        )\n",
    "        discriminator_optimizer.apply_gradients(\n",
    "            zip(discriminator_gradients, discriminator.trainable_variables)\n",
    "        )\n",
    "\n",
    "    return gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss\n",
    "\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    n = dataset.cardinality() # The number of batches per epoch\n",
    "    # Iterate over all batches\n",
    "    batch_d_losses = []\n",
    "    batch_g_losses = []\n",
    "    for i, batch in enumerate(dataset):\n",
    "        # Updte parameters for this batch\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss = train_step(batch)\n",
    "\n",
    "        # Store losses for batch, we will average these for the whole epoch for a more stable visualization\n",
    "        batch_d_losses.append(disc_loss)\n",
    "        batch_g_losses.append(gen_total_loss)\n",
    "\n",
    "        g_l, d_l = gen_total_loss.numpy(), disc_loss.numpy()\n",
    "        print(f\"Epoch {epoch+1} â€“ image {i+1}/{n} [G total loss: {g_l} | D loss: {d_l}]\", end=\"\\r\")\n",
    "\n",
    "    # Book-keeping:\n",
    "    # Visualize losses and one example image for the epoch\n",
    "    g_losses.append(np.mean(batch_g_losses))\n",
    "    d_losses.append(np.mean(batch_d_losses))\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.title('Losses')\n",
    "    plt.plot(np.array(d_losses)*40, label='Discriminator')\n",
    "    plt.plot(g_losses, label='Generator')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(model_dir, \"losses.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Save some example images and store model file\n",
    "    if epoch % save_interval == 0:\n",
    "        print('Saving epoch %d to %s'%(epoch+1, model_dir))\n",
    "        for j, (example_input, example_target) in enumerate(dataset.take(3)):\n",
    "            generate_images(\n",
    "                generator,\n",
    "                example_input,\n",
    "                example_target,\n",
    "                fname=os.path.join(generated_dir, f\"e{epoch+1:03}d_generated_image_{j+1}.png\")\n",
    "            )\n",
    "        generator.save(os.path.join(model_dir, f\"e{epoch+1:03}_generator.hd5\"), save_format='h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "org": null,
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
