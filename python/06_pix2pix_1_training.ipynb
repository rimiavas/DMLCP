{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lu0Wn4KjZAkI"
   },
   "source": [
    "# Pix2pix: Image to Image Translation\n",
    "\n",
    "Dataset preparation & training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrGLbKZawbeJ"
   },
   "source": [
    "\n",
    "Notebook adapted originally from [this tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix), ported to PyTorch integrating changes from [DMLAP](https://github.com/IriniKlz/DMLAP-2024/tree/main/python/06-GANs-pix2pix).\n",
    "\n",
    "See also [the original repo](https://github.com/phillipi/pix2pix) – in Lua – and the 'official' [PyTorch port](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/master) as well as the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ct96WoFbZAkQ",
    "outputId": "e5c5d76e-f42d-44b2-8cea-8dd656c4cd3b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from skimage import filters\n",
    "from skimage import feature\n",
    "from skimage import transform\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Get cpu, gpu or mps device for training\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# for dataset processing below\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import Lock\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing import set_start_method\n",
    "\n",
    "# make sure we are forking (hacky, for M1/2/3)\n",
    "if device == \"mps\":\n",
    "    set_start_method(\"fork\")\n",
    "\n",
    "print(f\"Multiprocessing: found {cpu_count()} CPUs\")\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fF6UiHGBZAkX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "IMG_SIZE = 256\n",
    "IMG_CHANNELS = 3\n",
    "IMG_EXTENSION = \".png\" # check directory and change to e.g. jpg instead!\n",
    "\n",
    "# fixed directory structure ----------------------------------------------------\n",
    "DATASETS_DIR = pathlib.Path(\"datasets\")\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "GENERATED_DIR = pathlib.Path(\"generated\")\n",
    "GENERATED_DIR.mkdir(exist_ok=True)\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCPvlcadZAkW",
    "tags": []
   },
   "source": [
    "## Datasets: Download and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFvPb-GbiROu"
   },
   "source": [
    "### Choose a Pix2pix Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZnN8hiOiUKs"
   },
   "source": [
    "**Original datasets**\n",
    "\n",
    "- cityscapes.tar.gz\n",
    "- edges2handbags.tar.gz\n",
    "- edges2shoes.tar.gz\n",
    "- facades.tar.gz\n",
    "- maps.tar.gz\n",
    "- night2day.tar.gz\n",
    "\n",
    "Available [here](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/). Right click on a link, 'copy link'. Beware the sizes! Some of them are pretty big.\n",
    "\n",
    "**Kaggle**\n",
    "\n",
    "- [Comic faces](https://www.kaggle.com/datasets/defileroff/comic-faces-paired-synthetic): `defileroff/comic-faces-paired-synthetic`\n",
    "- [Rembrandt](https://www.kaggle.com/datasets/grafstor/rembrandt-pix2pix-dataset): `grafstor/rembrandt-pix2pix-dataset`\n",
    "- [Depth](https://www.kaggle.com/datasets/greg115/pix2pix-depth): `greg115/pix2pix-depth`\n",
    "-  [Maps](https://www.kaggle.com/datasets/alincijov/pix2pix-maps) (`alincijov/pix2pix-maps`) (looks identical to `maps` above)\n",
    "\n",
    "There might be more! Also, if you want to use the transformation pipeline below, any image dataset would work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_F2I9dabiER8"
   },
   "source": [
    "### Directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bhCbATxis1C"
   },
   "source": [
    "Our goal is to create either one of the two directory structures.\n",
    "\n",
    "Either images inside our pix2pix directly:\n",
    "\n",
    "```bash\n",
    "datasets\n",
    "└── pix2pix_dataset_name\n",
    "    ├── 1.jpg\n",
    "    ...\n",
    "    └── 1198.jpg\n",
    "```\n",
    "\n",
    "Or the same, but with `train`, `test` or `val` sub-directories:\n",
    "\n",
    "```bash\n",
    "datasets\n",
    "└── pix2pix_dataset_name\n",
    "    ├── train\n",
    "    │   ├── 1.jpg\n",
    "    │   ├── 2.jpg\n",
    "    ...\n",
    "    ├── test\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Working with transformations**\n",
    "\n",
    "\n",
    "However, if we apply transformations to the dataset we download (like turning source images black and white), then we might want to keep the downloaded dataset under another name, so that our final, ready dataset is still `pix2pix_dataset_name`, for instance:\n",
    "\n",
    "```bash\n",
    "datasets\n",
    "├── pix2pix_dataset_name\n",
    "└── pix2pix_dataset_name_orig\n",
    "```\n",
    "\n",
    "**Datasets with source and target directories**\n",
    "\n",
    "For the comics faces dataset, for instance, the sources (pictures) and targets (comics) are in two separate subfolders, we will have to take that into account.\n",
    "\n",
    "```bash\n",
    "datasets\n",
    "├── pix2pix_dataset_name\n",
    "└── pix2pix_dataset_name_orig\n",
    "    ├── source\n",
    "    │   ├── 100.jpg\n",
    "    ...\n",
    "    ├── target\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNqd7aNdJO8D"
   },
   "source": [
    "\n",
    "### Note on Colab workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvN7iGLkJP5c"
   },
   "source": [
    "\n",
    "I recommend working in the same way as with the DCGAN:\n",
    "\n",
    "1. Either find a reliable url you can download a zip from, and use `wget` directly in the notebook, followed by:  \n",
    "\n",
    "    ```bash\n",
    "    unzip -q downloaded-file.zip -d <target-dir>\n",
    "    ```\n",
    "     \n",
    "    \n",
    "    or:\n",
    "   \n",
    "    ```\n",
    "    tar xzf downloaded-file.tar.gz -C <target-dir>\n",
    "    ```\n",
    "     \n",
    "    \n",
    "2. Or download the dataset locally first, then upload it to your drive, change the accessibility settings for the zip file in the drive, copy the `ID` and use `gdown` with the `ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjNTeY7BzSA-",
    "tags": []
   },
   "source": [
    "To save generated images and model checkpoints to your drive, it's best to connect to your drive *after* downloading the dataset.\n",
    "\n",
    "```python\n",
    "# reminder: Colab code to mount your drive\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    # Then you can access its contents, with '/drive/MyDrive' as the default for Google Drives\n",
    "```\n",
    "\n",
    "After that, you can `cp` and `mv` files to and from your drive as if they were on the local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw0ot0LnJHwm"
   },
   "source": [
    "### Web download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nN3XA6rJJus"
   },
   "source": [
    "Perhaps the easiest way to get the dataset is to use Unix tools:\n",
    "\n",
    "```python\n",
    "# download to datasets dir\n",
    "!wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz -P {DATASETS_DIR}\n",
    "\n",
    "# uncompress (still in datasets dir)\n",
    "!tar xzf {DATASETS_DIR}/facades.tar.gz -C {DATASETS_DIR}\n",
    "\n",
    "# rename to the uncompressed dir to right format: pix2pix_{name}\n",
    "# remember: our goal is to have either just the pictures in\n",
    "# `pix2pix_{MODEL_NAME}, or folders like `train`/`test`/`val`\n",
    "!mv {DATASETS_DIR}/facades {DATASETS_DIR}/pix2pix_facades\n",
    "\n",
    "# clean-up, !rm -r for directories\n",
    "# !rm {DATASETS_DIR}/facades.tar.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz1VFPRi2-i9"
   },
   "source": [
    "\n",
    "Below, our **global variables** will be:\n",
    "\n",
    "```python\n",
    "MODEL_NAME = \"facades\" # change accordingly (e.g. facades, bw2comics, etc.)\n",
    "\n",
    "# source and processed datasets\n",
    "PIX2PIX_DS_DIR_ORIG = DATASETS_DIR / f\"pix2pix_{MODEL_NAME}_orig\"\n",
    "# no need to create the dir now, it will be later\n",
    "\n",
    "PIX2PIX_DS_DIR = DATASETS_DIR / f\"pix2pix_{MODEL_NAME}\"\n",
    "# no need to create it now, it will be later\n",
    "\n",
    "PIX2PIX_DIR = MODELS_DIR / f\"pix2pix_{MODEL_NAME}\"\n",
    "PIX2PIX_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PIX2PIX_GEN_DIR = GENERATED_DIR / f\"pix2pix_{MODEL_NAME}_images\"\n",
    "PIX2PIX_GEN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Check when plotting or open an image manually!\n",
    "# 0: [target, source]\n",
    "# 1: [source, target]\n",
    "TARGET_INDEX = 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2JD5xYYw20i"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uOedZB8lcQC"
   },
   "source": [
    "### Kaggle download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhyXWEgHlfRb"
   },
   "source": [
    "To download a dataset from Kaggle, follow these steps:\n",
    "\n",
    "1. Create a Kaggle account (free)\n",
    "2. Go to [settings](https://www.kaggle.com/settings), create a token (`.json` file), download it locally and upload it to the main Colab folder (locally, you can just download the dataset manually). You can also save this file to your drive, and copy it from there like so:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# copy file from drive\n",
    "!cp \"drive/MyDrive/IS53024B-Artificial-Intelligence/kaggle.json\" .\n",
    "```\n",
    "Then  copy to the root of the Colab:\n",
    "```python\n",
    "if os.path.isfile(\"kaggle.json\"):\n",
    "    print(\"Found token file `kaggle.json`\")\n",
    "    !mkdir -p /root/.config/kaggle\n",
    "    !cp kaggle.json /root/.config/kaggle\n",
    "    !chmod 600 /root/.config/kaggle/kaggle.json\n",
    "else:\n",
    "    print(\"Could not token file `kaggle.json`, please see the steps above!\")\n",
    "```\n",
    "Then download:\n",
    "```python\n",
    "# also: \"grafstor/rembrandt-pix2pix-dataset\"\n",
    "ID = pathlib.Path(\"defileroff/comic-faces-paired-synthetic\") # on Kaggle: username/dataset\n",
    "!kaggle datasets download {ID} -p {DATASETS_DIR}\n",
    "```\n",
    "Unzip:\n",
    "```bash\n",
    "!unzip -q {DATASETS_DIR}/comic-faces-paired-synthetic.zip -d {DATASETS_DIR}\n",
    "```\n",
    "Clean-up:\n",
    "```bash\n",
    "# Then retrieve the name & structure, in this case:\n",
    "# face2comics_v1.0.0_by_Sxela/face2comics_v1.0.0_by_Sxela/\n",
    "# I want to simplify this, so:\n",
    "!mv {DATASETS_DIR}/face2comics_v1.0.0_by_Sxela/face2comics_v1.0.0_by_Sxela {DATASETS_DIR}/pix2pix_edges2comics_orig\n",
    "\n",
    "# # clean-up\n",
    "# !rm -r \"{DATASETS_DIR}/face2comics_v1.0.0_by_Sxela\"\n",
    "# !rm \"{DATASETS_DIR}/comic-faces-paired-synthetic.zip\"\n",
    "```\n",
    "\n",
    "Finally, change your **global variables** accordingly\n",
    "\n",
    "```python\n",
    "MODEL_NAME = \"face2comics\" # change accordingly (e.g. edge2comics, etc.)\n",
    "\n",
    "# source and processed datasets\n",
    "PIX2PIX_DS_DIR_ORIG = DATASETS_DIR / f\"pix2pix_{MODEL_NAME}_orig\"\n",
    "# PIX2PIX_DS_DIR_ORIG.mkdir(exist_ok=True) # already done, uncomment if you need\n",
    "\n",
    "PIX2PIX_DS_DIR = DATASETS_DIR / f\"pix2pix_{MODEL_NAME}\"\n",
    "# PIX2PIX_DS_DIR.mkdir(exist_ok=True) # already done, uncomment if you need\n",
    "\n",
    "PIX2PIX_DIR = MODELS_DIR / f\"pix2pix_{MODEL_NAME}\"\n",
    "PIX2PIX_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PIX2PIX_GEN_DIR = GENERATED_DIR / f\"pix2pix_{MODEL_NAME}_images\"\n",
    "PIX2PIX_GEN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# 0: [target, source]\n",
    "# 1: [source, target]\n",
    "TARGET_INDEX = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yysP-y5jw5Zj"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu-IDHkknvE5"
   },
   "source": [
    "### Setting up our global variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBM_NPqsv98y"
   },
   "source": [
    "Each training image in a standard pix2pix dataset consists of one imgage divided into two adjacent **source** and **target** images.\n",
    "The layout of the source and target may vary from training set to trainig set, so we provide a `TARGET_INDEX` flag the determines on which side the target is (`0` if on the left and `1` if on the right). Set this so the examples from the dataset appear with the source image to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBUIE_HF3iWu"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOc-_oP8qQ7Z"
   },
   "source": [
    "## Working with transformations\n",
    "\n",
    "This entire section can be ignored if our dataset is already `pix2pix` formatted (with AB or BA images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu4_AZu7rp_n"
   },
   "source": [
    "### More Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm0QIDMBwL0p"
   },
   "source": [
    "\n",
    "This section is meant for two scenarios:\n",
    "\n",
    "1. We have two separate directories with A and B images, they will be:\n",
    "    - `SOURCE_DIR`\n",
    "    - `TARGET_DIR`\n",
    "\n",
    "2. We only want to transform one set of images, and that will be:\n",
    "    - `TARGET_DIR`\n",
    "    - (`SOURCE_DIR = None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g34EvyDArs0R"
   },
   "outputs": [],
   "source": [
    "# For a standard pix2pix dataset (source/target in one image, set to True)\n",
    "IS_INPUT_PIX_TO_PIX = False\n",
    "\n",
    "# Only used if we have separate source and target directories,\n",
    "# e.g. 'datasets/pix2pix_face2comics/face/'\n",
    "SOURCE_DIR = PIX2PIX_DS_DIR_ORIG / \"face\"\n",
    "TARGET_DIR = PIX2PIX_DS_DIR_ORIG / \"comics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uvv2IG6pibtz"
   },
   "source": [
    "### Load the images to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ry9Ytnibtz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    w, h = (256, 256)\n",
    "    if IS_INPUT_PIX_TO_PIX: # In case we are already loading a pix2pix image\n",
    "        w, h = (512, 256)\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, (w,h), interpolation=cv2.INTER_NEAREST)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # OpenCV is BGR by default\n",
    "    # If we are loading a pix2pix dataset just extract the target\n",
    "    if IS_INPUT_PIX_TO_PIX:\n",
    "        if TARGET_INDEX == 0:\n",
    "            img = img[:,:h,:]\n",
    "        else:\n",
    "            img = img[:,h:,:]\n",
    "\n",
    "    return img\n",
    "\n",
    "def load_images_in_path(path, shuffle=False, limit=0):\n",
    "    fnames = glob.glob(os.path.join(path, \"*\"))\n",
    "    print(f\"Found {len(fnames)} files in '{path}'\")\n",
    "    if limit > 0:\n",
    "        fnames = fnames[:limit]\n",
    "        print(f\"Limiting number of files to {limit}\")\n",
    "    for f in fnames:\n",
    "        yield load_image(f) # See this: https://realpython.com/introduction-to-python-generators/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "id": "MJRhjk8wibtz",
    "outputId": "c7f2741d-99ff-44b8-9a6f-54899cdd1a0d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SOURCE_DIR:\n",
    "    source_loader = iter(load_images_in_path(SOURCE_DIR))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(next(source_loader))\n",
    "    plt.show()\n",
    "\n",
    "target_loader = iter(load_images_in_path(TARGET_DIR)) # create an iterator\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(next(target_loader))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqb7xJWQibt0"
   },
   "source": [
    "### The transformation pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6mvPVGytKB8"
   },
   "source": [
    "#### Transformation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Y5OA-WEibt1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_bw_cv2(img):\n",
    "    \"\"\"Turn an image black and white using OpenCV\"\"\"\n",
    "    grey_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.merge([grey_img, grey_img, grey_img]) # Force three channels for shape compat, thanks ChatGPT!\n",
    "\n",
    "def apply_canny_cv2(img, thresh1=160, thresh2=250, invert=False):\n",
    "    \"\"\"Apply the OpenCV Canny edge detector to an image\"\"\"\n",
    "    grey_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(grey_img, thresh1, thresh2)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "def apply_canny_skimage(img, sigma=1.5, invert=False):\n",
    "    \"\"\"Apply the Scikit-Image Canny edge detector to an image\"\"\"\n",
    "    grey_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = (feature.canny(grey_img, sigma=sigma)*255).astype(np.uint8)\n",
    "    if invert:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# IDEA: It might be possible to use other Mediapipe functionalities, like:\n",
    "#       - segmentation: https://developers.google.com/mediapipe/solutions/vision/image_segmenter\n",
    "#       - pose landmarks: https://developers.google.com/mediapipe/solutions/vision/pose_landmarker\n",
    "#       to write other transformation functions... (For both of those, you then need to find datasets!)\n",
    "\n",
    "def apply_face_landmarks(\n",
    "        img, stroke_weight=2, overlay=False, overlay_color='black'\n",
    "    ):\n",
    "    \"\"\"Apply the MediaPipe face landmarker to an image\"\"\"\n",
    "    import urllib\n",
    "    import mediapipe as mp # requires pip install mediapipe\n",
    "    from mediapipe import solutions\n",
    "    from mediapipe.framework.formats import landmark_pb2\n",
    "    from mediapipe.tasks.python import vision\n",
    "    from mediapipe.tasks.python.core import base_options as base_options_module\n",
    "\n",
    "    # Path to the model file\n",
    "    model_path = PIX2PIX_DIR / \"face_landmarker.task\"\n",
    "\n",
    "    # Check if the model file exists, if not, download it\n",
    "    if not model_path.exists():\n",
    "        url = \"https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\"\n",
    "        print(f\"Downloading model from {url}...\")\n",
    "        urllib.request.urlretrieve(url, model_path)\n",
    "        print(f\"Model downloaded and saved as {model_path}\")\n",
    "\n",
    "    # Initialize MediaPipe FaceLandmarker\n",
    "    base_options = base_options_module.BaseOptions(model_asset_path=model_path)\n",
    "    options = vision.FaceLandmarkerOptions(\n",
    "        base_options=base_options,\n",
    "        output_face_blendshapes=True,\n",
    "        output_facial_transformation_matrixes=True,\n",
    "        num_faces=1\n",
    "    )\n",
    "    detector = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "    # Function to draw landmarks on the image\n",
    "    def draw_landmarks_on_image(rgb_image, detection_result,\n",
    "                                overlay=False, overlay_color='black'):\n",
    "        face_landmarks_list = detection_result.face_landmarks\n",
    "\n",
    "        if overlay:\n",
    "            annotated_image = np.copy(rgb_image)\n",
    "        else:\n",
    "            if overlay_color == \"white\":\n",
    "                annotated_image = np.ones_like(rgb_image) * 255\n",
    "            else: # default to black\n",
    "                annotated_image = np.zeros_like(rgb_image)\n",
    "\n",
    "        # Loop through the detected faces to visualize.\n",
    "        for idx in range(len(face_landmarks_list)):\n",
    "            face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "            # Draw the face landmarks.\n",
    "            face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "            face_landmarks_proto.landmark.extend([\n",
    "                landmark_pb2.NormalizedLandmark(\n",
    "                    x=landmark.x, y=landmark.y, z=landmark.z\n",
    "                ) for landmark in face_landmarks\n",
    "            ])\n",
    "\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=solutions.drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=solutions.drawing_styles.get_default_face_mesh_contours_style())\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=solutions.face_mesh.FACEMESH_IRISES,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=solutions.drawing_styles.get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "        return annotated_image\n",
    "\n",
    "    # Convert the frame to RGB and create MediaPipe Image\n",
    "    rgb_frame = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "    # Detect face landmarks in the frame\n",
    "    detection_result = detector.detect(mp_image)\n",
    "\n",
    "    # Annotate frame with detected landmarks\n",
    "    if detection_result:\n",
    "        annotated_img = draw_landmarks_on_image(\n",
    "            img, detection_result, overlay=overlay, overlay_color=overlay_color\n",
    "    )\n",
    "    else: # you could also imagine returning a purely black/white image\n",
    "        annotated_img = img\n",
    "\n",
    "    return annotated_img\n",
    "\n",
    "# IDEA: Use Canvas (or openCV) to remove parts of the image (draw a rectangle/circle somewhere)\n",
    "#       so that the net learns to complete an image with a hole in it (inpainting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmzKDZRgvkse"
   },
   "source": [
    "#### Select our transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pivB3ji1tBwR"
   },
   "outputs": [],
   "source": [
    "# As it is, this version loads an image from the source_image directory and\n",
    "# applies the Canny edge detection algorithm to it.\n",
    "def load_source(img, img_source_iterator):\n",
    "    return next(img_source_iterator)\n",
    "\n",
    "# Set this to the tranformation you want to apply. If you are only working with\n",
    "# a single folder of images that you want to process, set image_transformation\n",
    "# to one of the filtering operations above,\n",
    "# e.g.\n",
    "# image_transformation = apply_bw_cv2\n",
    "image_transformation = apply_canny_cv2\n",
    "# image_transformation = apply_canny_skimage\n",
    "# image_transformation = apply_face_landmarks\n",
    "\n",
    "# If you are working with existing sources, you can use\n",
    "# a Python partial to assign a fixed argument to load_source\n",
    "# and use it exactly like the other image transformations\n",
    "# (See: https://docs.python.org/3/library/functools.html#functools.partial)\n",
    "\n",
    "# from functools import partial\n",
    "# image_transformation = partial(load_source, img_source_iterator=iter(load_images_in_path(SOURCE_DIR)))\n",
    "\n",
    "# this can be used to specify arguments for `apply_face_landmarks`\n",
    "# from functools import partial\n",
    "# image_transformation = partial(apply_face_landmarks, overlay_color=\"white\")\n",
    "\n",
    "img = next(load_images_in_path(TARGET_DIR))\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image_transformation(img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJLkDvd0ibt1"
   },
   "source": [
    "#### Create the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Phluf5Vnibt2",
    "outputId": "bd4fdae6-20db-4b49-8a7c-8873e686b4f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_IMAGES = 0     # max images to process, 0: disabled\n",
    "PRINT_EVERY = 1000 # printing progress\n",
    "\n",
    "img_loader = iter(load_images_in_path(TARGET_DIR, limit=MAX_IMAGES))\n",
    "\n",
    "def combine_images(source, target):\n",
    "    if TARGET_INDEX == 1:\n",
    "        combined = np.hstack([source, target])\n",
    "    else:\n",
    "        combined = np.hstack([target, source])\n",
    "    return combined\n",
    "\n",
    "def process_source_target(i, source, target, out_dir):\n",
    "    \"\"\"Combine source and target and save them\"\"\"\n",
    "    # IDEA: you could also apply additional processing to either\n",
    "    # your source or target here\n",
    "    # target = image_transformation(target)\n",
    "    # target = image_transformation(source)\n",
    "    combined = combine_images(source, target)\n",
    "    combined = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)\n",
    "    # multiprocessing: only one process at a time\n",
    "    with lock:\n",
    "        cv2.imwrite(out_dir / f\"{i+1}.png\", combined)\n",
    "\n",
    "def process_target(i, target, out_dir):\n",
    "    \"\"\"Process target and save the combine images\"\"\"\n",
    "    source = image_transformation(target)\n",
    "    combined = combine_images(source, target)\n",
    "    combined = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)\n",
    "    # multiprocessing: only one process at a time\n",
    "    with lock:\n",
    "        cv2.imwrite(out_dir / f\"{i+1}.png\", combined)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# multiprocessing!\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "l = Lock()\n",
    "def init(l):\n",
    "    \"\"\"Initialize a lock to avoid race conditions\"\"\"\n",
    "    global lock\n",
    "    lock = l\n",
    "\n",
    "if SOURCE_DIR:\n",
    "    # CASE 1: we have source images, we want to combine them\n",
    "    PIX2PIX_DS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    # create a source image iterator\n",
    "    source_loader = iter(load_images_in_path(SOURCE_DIR, limit=MAX_IMAGES))\n",
    "\n",
    "    with Pool(processes=cpu_count(), initializer=init, initargs=(l,)) as p:\n",
    "        for i, (source, target) in enumerate(zip(source_loader, img_loader)):\n",
    "            if (i+1) % PRINT_EVERY == 0:\n",
    "                print(f\"Processing source #{i+1}\")\n",
    "            p.apply_async(\n",
    "                process_source_target, args=(i, source, target, PIX2PIX_DS_DIR)\n",
    "            )\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "else:\n",
    "    # CASE 2: we only have targets, we create the sources and combine them\n",
    "    PIX2PIX_DS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    with Pool(processes=cpu_count(), initializer=init, initargs=(l,)) as p:\n",
    "        for i, target in enumerate(img_loader):\n",
    "            if (i+1) % PRINT_EVERY == 0:\n",
    "                print(f\"Processing target #{i+1}\")\n",
    "            p.apply_async(process_target, args=(i, target, PIX2PIX_DS_DIR))\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "print(\"Total time:\", time.time() - t)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# verify the number of files in our directory (wc counts words, lines or bytes)\n",
    "!ls {PIX2PIX_DS_DIR} | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LC-orP-zSA9",
    "tags": []
   },
   "source": [
    "## Dataset ready: load and preprocess it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-HopzNiZAkY"
   },
   "source": [
    "The following code also **augments** the dataset by applying random uniform scaling (by upscaling and cropping) and random mirroring to the input output pairs. This should lead to a more stable model according to the original pix2pix paper. Finally the images ar normalized to the [-1,1] range as required by our GAN-based model.\n",
    "\n",
    "We will organize the dataset in batches of size `1`, as that is generally suggested for pix2pix models. That means that we will update the weights of the model for each image pair separately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgJnwspntxXX"
   },
   "outputs": [],
   "source": [
    "def random_jitter(input_image, target_image):\n",
    "    # Resizing to 286x286\n",
    "    resize_transform = v2.Resize(size=(286, 286), interpolation=v2.InterpolationMode.NEAREST)\n",
    "    input_image = resize_transform(input_image)\n",
    "    target_image = resize_transform(target_image)\n",
    "\n",
    "    # Random cropping back to 256x256\n",
    "    i, j, h, w = v2.RandomCrop.get_params(input_image, output_size=(256, 256))\n",
    "    input_image = TF.crop(input_image, i, j, h, w)\n",
    "    target_image = TF.crop(target_image, i, j, h, w)\n",
    "\n",
    "    # Random mirroring\n",
    "    if np.random.uniform() < 0.5:\n",
    "        input_image = TF.hflip(input_image)\n",
    "        target_image = TF.hflip(target_image)\n",
    "\n",
    "    return input_image, target_image\n",
    "\n",
    "class Pix2PixImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, target_index):\n",
    "        super(Pix2PixImageDataset, self).__init__()\n",
    "        self.files = [os.path.join(path, f) for f in os.listdir(path) if '.jpg' in f or '.png' in f]\n",
    "        self.target_index = target_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            shape = (BATCH_SIZE, IMG_CHANNELS, IMG_SIZE, IMG_SIZE)\n",
    "            return torch.zeros(shape).to(device), torch.zeros(shape).to(device)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # OpenCV is BGR by default\n",
    "        image = v2.ToImage()(image)\n",
    "        image = v2.ToDtype(torch.float32, scale=True)(image) # from [0,255] to [0,1]\n",
    "        w = image.shape[-1]\n",
    "        w = w // 2\n",
    "\n",
    "        if self.target_index == 0:          # 0: [target, source]\n",
    "            input_image = image[:, :, w:]\n",
    "            target_image = image[:, :, :w]\n",
    "        else:                               # 1: [source, target]\n",
    "            input_image = image[:, :, :w]\n",
    "            target_image = image[:, :, w:]\n",
    "        # Jitter\n",
    "        input_image, target_image = random_jitter(input_image, target_image)\n",
    "        # Normalize\n",
    "        input_image = input_image * 2 - 1\n",
    "        target_image = target_image * 2 - 1\n",
    "        return input_image.to(device), target_image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Xs3sO_jEtm1",
    "outputId": "b6f90253-1118-41ff-bb41-20087063ab1f"
   },
   "outputs": [],
   "source": [
    "split_train_test = True # change accordingly!\n",
    "# only used if `split_train_test` is False (a split already present)\n",
    "TRAIN_DIR = \"train\"\n",
    "TEST_DIR = \"val\"\n",
    "\n",
    "if split_train_test:\n",
    "    train_data_orig = Pix2PixImageDataset(PIX2PIX_DS_DIR, TARGET_INDEX)\n",
    "\n",
    "    train_data, test_data = torch.utils.data.random_split(train_data_orig, [.9,.1])\n",
    "    print(f\"Train samples: {len(train_data)} | Test samples: {len(test_data)}\")\n",
    "else:\n",
    "    train_data = Pix2PixImageDataset(PIX2PIX_DS_DIR / TRAIN_DIR, TARGET_INDEX)\n",
    "    test_data = Pix2PixImageDataset(PIX2PIX_DS_DIR / TEST_DIR, TARGET_INDEX)\n",
    "\n",
    "# create data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "A6JGsy7KJSJI",
    "outputId": "eb9cce9c-8e9a-49d0-b0cf-0984ce3eddbe"
   },
   "outputs": [],
   "source": [
    "# get a batch of training images\n",
    "for i, t in enumerate(train_dataloader):\n",
    "\n",
    "    x, y = t\n",
    "\n",
    "    plt.figure(figsize=(13,16))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Training Images\")\n",
    "    lim = 16\n",
    "    plt.imshow(TF.to_pil_image(\n",
    "        tv.utils.make_grid(\n",
    "            torch.cat([x[:lim], y[:lim]], dim = -1),\n",
    "            # x[:64],\n",
    "            padding=2, normalize=True, nrow=4\n",
    "        ).detach().cpu())\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhkcyQp_6fiN"
   },
   "source": [
    "Note: sometimes you run into errors because of faulty files, etc. One way to test that everything is right in your dataset is to loop over it once, doing nothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uR8hDRg70kDy"
   },
   "outputs": [],
   "source": [
    "for i, (input_image, target_image) in enumerate(train_dataloader):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DU1NA1qsZAkb"
   },
   "source": [
    "## Build  the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F50usLYaZAkc"
   },
   "source": [
    "The pix2pix model is a conditional generative adversarial network (cGAN). A cGAN\n",
    "is a type of GAN model used for generating new data samples with specific\n",
    "attributes or characteristics. In a cGAN, both the generator and discriminator\n",
    "are *conditioned* on additional information, such as class labels, tags, or\n",
    "other types of metadata. The generator network takes in random noise as well as\n",
    "the conditional information as input and produces a new data sample that matches\n",
    "the desired attributes. The discriminator network, on the other hand, tries to\n",
    "distinguish between the generated samples and real samples based on both their\n",
    "visual appearance and the conditional information. For the case of a pix2pix\n",
    "model the network is conditioned on an image, which should be transformed into\n",
    "an output image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr7RGrPqZAkd"
   },
   "source": [
    "### Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2Y_7iSpZAke"
   },
   "source": [
    "Differently from a DC-GAN, the generator of the pix2pix model is based on the\n",
    "[U-net](https://arxiv.org/abs/1505.04597) architecture. A U-net model is a CNN architecture that is typically used\n",
    "for image segmentation tasks. The name U-net derives from the architecture,\n",
    "which resembles the letter &ldquo;U&rdquo;. It consists of two main parts: an *encoder* and\n",
    "a *decoder*. The encoder part consists of a series of convolutional layers,\n",
    "which reduce the spatial dimensionality of the input image while increasing its\n",
    "depth (`Conv2d`). This is followed by a bottleneck layer that extracts the most important\n",
    "features from the input image. The decoder part is a &ldquo;mirror image&rdquo; of the\n",
    "encoder. It consists of a series of layers that gradually increase the spatial\n",
    "dimensionality of the output, while decreasing its depth. This is similar to\n",
    "what we have seen in the DC-GAN example, also using a &ldquo;transposed\n",
    "convolution&rdquo; layer (`ConvTranspose2d`). The output of each layer in the encoder is\n",
    "also concatenated with the output of another layer in the decoder. This creates\n",
    "&ldquo;skip connections&rdquo; that help preserve spatial information and avoid information\n",
    "loss during the encoding and decoding process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2amP3ZGZAkf",
    "outputId": "89e6107f-d3d2-47bc-9b7c-a6a0ca8c62ba",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, size=4, stride=2, apply_batchnorm=True):\n",
    "        # Convolution-BatchNorm-ReLU\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, size, stride=stride, padding=1, bias=not apply_batchnorm)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels) if apply_batchnorm else None\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.batchnorm is not None:\n",
    "            x = self.batchnorm(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        return x\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, size=4, stride=2, apply_dropout=False):\n",
    "        # Convolution-BatchNorm-Dropout-ReLU\n",
    "        super().__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, size, stride=stride, padding=1, bias=True)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout(0.5) if apply_dropout else None\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_transpose(x)\n",
    "        x = self.batchnorm(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        # encoder:\n",
    "        # C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        # decoder with skip (in/out):\n",
    "        # CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "        # CD512-CD512 -CD512 -C512 -C256 -C128-C64\n",
    "\n",
    "        self.encoders = nn.ModuleList([\n",
    "            Downsample(3, 64, apply_batchnorm=False),\n",
    "            Downsample(64, 128),\n",
    "            Downsample(128, 256),\n",
    "            Downsample(256, 512),\n",
    "            Downsample(512, 512),\n",
    "            Downsample(512, 512),\n",
    "            Downsample(512, 512),\n",
    "            Downsample(512, 512, apply_batchnorm=False)\n",
    "        ])\n",
    "\n",
    "        self.decoders = nn.ModuleList([\n",
    "            Upsample(512, 512, apply_dropout=True),\n",
    "            Upsample(1024, 512, apply_dropout=True),\n",
    "            Upsample(1024, 512, apply_dropout=True),\n",
    "            Upsample(1024, 512),\n",
    "            Upsample(1024, 256),\n",
    "            Upsample(512, 128),\n",
    "        ])\n",
    "        self.last_decoder = Upsample(256, 64)\n",
    "        self.last = nn.ConvTranspose2d(64, img_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for i, down in enumerate(self.encoders):\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "        skips = skips[:-1][::-1]\n",
    "        for i, up in enumerate(self.decoders):\n",
    "            x = up(x)\n",
    "            x = torch.cat([x, skips[i]], dim=1) # residual layers\n",
    "        x = self.last_decoder(x)\n",
    "        x = self.last(x)\n",
    "        return torch.tanh(x)\n",
    "\n",
    "G = Generator().to(device)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r8TVlTFZAkg"
   },
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator in the pix2pix cGAN is a convolutional PatchGAN classifier—it tries to classify if each image _patch_ is real or not real, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
    "\n",
    "- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n",
    "- The shape of the output after the last layer is `(batch_size, 30, 30, 1)`.\n",
    "- Each `30 x 30` image patch of the output classifies a `70 x 70` portion of the input image.\n",
    "- The discriminator receives 2 inputs:\n",
    "    - The input image and the target image, which it should classify as real.\n",
    "    - The input image and the generated image (the output of the generator), which it should classify as fake.\n",
    "    - Use `torch.cat([inp, tar], dim=1)` to concatenate these 2 inputs together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d39MMY_ZAkh",
    "outputId": "78c4428e-ca61-4fdd-c0f3-a228bb30b683",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_channels=3):\n",
    "        # C64-C128-C256-C512\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.down1 = Downsample(image_channels*2, 64, 4, apply_batchnorm=False)\n",
    "        self.down2 = Downsample(64, 128, 4)\n",
    "        self.down3 = Downsample(128, 256, 4)\n",
    "        self.down4 = Downsample(256, 512, 4, stride=1)\n",
    "        self.last = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, inp, tar):\n",
    "        x = torch.cat([inp, tar], dim=1)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "        x = self.down4(x)\n",
    "        x = self.last(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "D = Discriminator().to(device)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNIkm1vZZAki"
   },
   "source": [
    "### Generate some images before training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3l4MgsTvZAki"
   },
   "source": [
    "Let&rsquo;s generate some images before training to see what the network will output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "bkjY3Ks3ZAkj",
    "outputId": "70fa3353-ad82-44bd-c9c1-3b101006fb04",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def numpy_image(x):\n",
    "    return np.transpose(x.detach().cpu().numpy(), (1, 2, 0)) * 0.5 + 0.5\n",
    "\n",
    "def generate_images(\n",
    "        model, inputs, target=None,\n",
    "        max_images=1, show=True, fname_no_ext=\"\"\n",
    "    ):\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    # print(prediction.size())\n",
    "\n",
    "    for i, (input, predicted_image) in enumerate(zip(inputs, outputs)):\n",
    "\n",
    "        # print(i)\n",
    "        if i >= max_images:\n",
    "            break\n",
    "\n",
    "        if target is not None:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            display_list = [input, target[i], predicted_image]\n",
    "            title = ['Input Image', 'Target', 'Predicted Image']\n",
    "        else:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            display_list = [input, predicted_image]\n",
    "            title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "        for j in range(len(title)):\n",
    "            plt.subplot(1, len(title), j+1)\n",
    "            plt.title(title[j])\n",
    "            # Getting the pixel values in the [0, 1] range to plot.\n",
    "            plt.imshow(numpy_image(display_list[j]))\n",
    "            plt.axis('off')\n",
    "\n",
    "        if fname_no_ext:\n",
    "            plt.savefig(f\"{fname_no_ext}_{i:04d}.png\")\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "for i, (example_input, example_target) in enumerate(train_dataloader):\n",
    "    generate_images(G, example_input, example_target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO1eQZSVZAkj"
   },
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNHgIcrr_g3u"
   },
   "source": [
    "This follows the procedure described in the [pix2pix paper](https://arxiv.org/abs/1611.07004). Similarly to unconditional GANs, this conditional GAN (cGAN) is learning to map edges to photo.\n",
    "\n",
    "The discriminator D learns to classify between fake (synthesised by generator) and real {edges, photo} tuples. The generator G learns to fool the discriminator.\n",
    "\n",
    "Unlike an unconditional GAN, here, both G and D observe the input edge map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHP5n87UZAkm"
   },
   "source": [
    "### Training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2WZJ7GeZAkm"
   },
   "source": [
    "The training loop procedes by separately optimizing the discriminator and generator at each iteration. The procedure can be summarized as follows:\n",
    "- For each example input we use the Generator to generate an output.\n",
    "- Update the discriminator by:\n",
    "    -  (1) Feeding it the input image and the example target image to classify the ground truth (example) pair.\n",
    "    -  (2) Feeding it the input image together with the generated output to classify the generated pair.\n",
    "    -  Using these two outputs (1 and 2) to compute the discriminator loss and to update the discriminator parameters to minimize this loss. In order to update only the discriminator, when computing step (2) the generated image is \\\"detached\\\" (using the `.detach()` function) from the Torch computation graph, so that the gradients will not be \\\"frozen\\\" and not propagated back to the generator. \n",
    "- Update the generator by:\n",
    "    -  Computing (2) again with the updated discriminator but this time without detaching the generated image\n",
    "    -  Computing the generator loss by combining the classification loss computed for the discriminator and the [L1 distance](https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c) between the generated image and the target one and finally updating the parameters of the generator to minimize this loss.\n",
    " \n",
    "The full structure:\n",
    "\n",
    "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwe4lJJmZAkk"
   },
   "source": [
    "#### Generator loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1IQaX1g_efy"
   },
   "source": [
    "While GANs learn a loss that adapts to the data, cGANs (as Pix2Pix) learn a structured loss that penalizes a possible structure that differs from the network output and the target image. The generator loss consists of two terms:\n",
    "\n",
    "-   Similarly to the discriminator case, the first term `fake_gan_loss` is a sigmoid cross-entropy loss of the (discriminated) generated images and an array of ones, i.e. considering the generated output as a real sample.\n",
    "-   The second term `dist_loss` quantifies the L1 distance, i.e. the mean absolute error (absolute value of differences), between the generated image and the target image. This allows the generated image to become structurally similar to the target image.\n",
    "-   These two terms are combined as `fake_gan_loss + LAMBDA * dist_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper.\n",
    "Feel free to experiment with modifying the value of `LAMBDA` (if you have time to spare:))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ijt2pUZAkl"
   },
   "source": [
    "#### Discriminator loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFRSCu6IZAkl"
   },
   "source": [
    "The discriminator loss (`disc_loss`) consists of the average of two terms, a `real_loss` and a `fake_loss`:\n",
    "- The `real_loss` is the is a [binary cross-entropy loss](https://gombru.github.io/2018/05/23/cross_entropy_loss/) of the (discriminated) real images and an array of ones (since these are the real images). \n",
    "- The `fake_loss` is the is a binary cross-entropy loss of the (discriminated) fake images and an array of zeros (since these are the fake images). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXUZNe1xTRir"
   },
   "outputs": [],
   "source": [
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "BCE_loss = nn.BCELoss()\n",
    "L1_loss = nn.L1Loss()\n",
    "\n",
    "LAMBDA = 100 # Weight of L1 loss in optimization\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    # Generate output\n",
    "    G_outputs = G(inputs)\n",
    "\n",
    "    # ---- Update discriminator ----\n",
    "    D_optimizer.zero_grad() # Clear gradients\n",
    "\n",
    "    # Classify real and fake patches\n",
    "    # Here we \"freeze\" generator gradients since we only optimize the discriminator\n",
    "    real_patch = D(inputs, targets)\n",
    "    fake_patch = D(inputs, G_outputs.detach())\n",
    "\n",
    "    # Compute loss for real/fake patches\n",
    "    # log(D(x,y)) + log(1 - D(x,G(x)))\n",
    "    real_class = torch.ones_like(real_patch).to(device)\n",
    "    fake_class = torch.zeros_like(fake_patch).to(device)\n",
    "\n",
    "    real_loss = BCE_loss(real_patch, real_class)\n",
    "    fake_loss = BCE_loss(fake_patch, fake_class)\n",
    "    D_loss = (real_loss + fake_loss)/2\n",
    "\n",
    "    # Propagate gradients and perform gradient descent step\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "\n",
    "    # ---- Update generator ----\n",
    "    G_optimizer.zero_grad() # Clear gradients\n",
    "    # Classify fake samples, now considering generator gradients\n",
    "    fake_patch = D(inputs, G_outputs)\n",
    "    # Compute loss according to paper\n",
    "    # log(D(x,G(x))) + L1(y,G(x))\n",
    "    fake_gan_loss = BCE_loss(fake_patch, real_class)\n",
    "    l1_loss = L1_loss(targets, G_outputs)\n",
    "    G_loss = fake_gan_loss + LAMBDA * l1_loss\n",
    "\n",
    "    # Propagate gradients and perform gradient descent step\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "\n",
    "    return G_loss, D_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auYiI7zihIfz"
   },
   "outputs": [],
   "source": [
    "def plot(d_losses, g_losses, show=True, save=False):\n",
    "    \"\"\"\n",
    "    Book-keeping: visualize losses and one example image for the epoch\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.title('Losses')\n",
    "    plt.plot(g_losses, label='Generator')\n",
    "    plt.legend()\n",
    "    plt.plot(np.array(d_losses), label='Discriminator')\n",
    "    plt.legend()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    if save:\n",
    "        plt.savefig(PIX2PIX_DIR / \"losses.pdf\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7-RU8O2Vq6b",
    "outputId": "a86aa7b0-e216-4fdd-ffc8-c5565451b28e"
   },
   "outputs": [],
   "source": [
    "tot = len(train_dataloader) # for print formatting\n",
    "print(f\"The train dataset contains {tot} batches (# of iterations/epoch)!\")\n",
    "# Using this information, we can modify the variables below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvOnly6Pmriu"
   },
   "outputs": [],
   "source": [
    "# Those will contain our losses (keeping them in a different cell\n",
    "# allows them to persist as we run the next cell several times)\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N3WEvSb_TB7O",
    "outputId": "a663426c-dee8-44d9-ddd4-5c5bdd42d96a"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "# All stats according to iters, rather than batch or epoch\n",
    "AVG_EVERY = 10      # collect batch losses for plotting\n",
    "PRINT_EVERY = 100   # print stats\n",
    "GEN_EVERY = 2000    # generate imgs\n",
    "PLOT_EVERY = 2000   # plot losses (skips iter 0 unless it is 1: 'every iter')\n",
    "                    # ↓ save model (skips iter 0 unless it is 1: 'every iter')\n",
    "SAVE_EVERY = iters + EPOCHS * tot * BATCH_SIZE # save at the very end!\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print()\n",
    "    print(f\"Epoch {epoch+1:>{len(str(EPOCHS))}}/{EPOCHS} | Iter: {iters+1}\")\n",
    "\n",
    "    batch_d_losses = []\n",
    "    batch_g_losses = []\n",
    "\n",
    "    for i, (input_image, target_image) in enumerate(train_dataloader):\n",
    "\n",
    "        G_loss, D_loss = train_step(input_image, target_image)\n",
    "        g_l, d_l = G_loss.item(), D_loss.item()\n",
    "        batch_g_losses.append(g_l)\n",
    "        batch_d_losses.append(d_l)\n",
    "\n",
    "        if (iters + 1) % PRINT_EVERY == 0:\n",
    "            print(\n",
    "                f\"  Iter {iters+1} (i {i+1:>{len(str(tot))}}/{tot}) \"\n",
    "                + f\"[G loss: {g_l:.4f} | D loss: {d_l:.4f}]\"\n",
    "            )\n",
    "\n",
    "        if iters % AVG_EVERY == 0:\n",
    "            # print(\"Averaging losses\")\n",
    "            # average values collected until now, reset temporary lists\n",
    "            g_losses.append(np.mean(batch_g_losses))\n",
    "            d_losses.append(np.mean(batch_d_losses))\n",
    "            batch_d_losses = []\n",
    "            batch_g_losses = []\n",
    "\n",
    "        if (iters + 1) % GEN_EVERY == 0:\n",
    "            generate_images(\n",
    "                G, input_image, target_image,\n",
    "                fname_no_ext=PIX2PIX_GEN_DIR / f\"generated_image.iter_{iters+1:04d}\"\n",
    "            )\n",
    "\n",
    "        if (iters > 0 or PLOT_EVERY == 1) and (iters + 1) % PLOT_EVERY == 0:\n",
    "            # print()\n",
    "            # print(f\"  Iter {iters + 1} | Plotting\")\n",
    "            # print()\n",
    "            plot(d_losses, g_losses)\n",
    "\n",
    "        if (iters > 0 or SAVE_EVERY == 1) and (iters + 1) % SAVE_EVERY == 0:\n",
    "            print()\n",
    "            print(f\"  Iter {iters+1} | Saving model to {PIX2PIX_DIR}\")\n",
    "            print()\n",
    "            G_scripted = torch.jit.script(G)\n",
    "            G_scripted.save(PIX2PIX_DIR / f\"pix2pix_{MODEL_NAME}.iter_{iters+1:04d}.pt\")\n",
    "            # The following saves only model parameters\n",
    "            #torch.save(G.state_dict(), PIX2PIX_DIR / f\"pix2pix_{MODEL_NAME}.iter_{iters+1:04d}.pt\")\n",
    "\n",
    "        iters += 1 # technically should be BATCH_SIZE, but oh well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyDQdY4Ee9TR"
   },
   "source": [
    "### Download your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TFmwKu9fFsC"
   },
   "source": [
    "```bash\n",
    "# -r for 'recursive', required for directories\n",
    "!zip -r generated.zip generated\n",
    "!zip -r models.zip models\n",
    "# then use the left-hand side bar to download manually.\n",
    "```\n",
    "\n",
    "(The process is similar for `datasets`, if you want to save that.)\n",
    "\n",
    "If you want to save things to your drive, you, can then move those to the destination of your choice (provided you mounted your drive in the first place):\n",
    "\n",
    "```bash\n",
    "!cp -r models/* !mv drive/MyDrive/IS53055B-DMLCP/DMLCP/python/models\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_F2I9dabiER8",
    "fNqd7aNdJO8D"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "org": null,
  "vscode": {
   "interpreter": {
    "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
